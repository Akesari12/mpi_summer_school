{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Machine Learning: Text Analysis and Text as Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anike\\anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\Anike\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.FB5AE2TYXYH2IJRDKGDGQ3XBKLKTF43H.gfortran-win_amd64.dll\n",
      "C:\\Users\\Anike\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.wcdjnk7yvmpzq2me2zzhjjrj3jikndb7.gfortran-win_amd64.dll\n",
      "C:\\Users\\Anike\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.21-gcc_10_3_0.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "# import libraries used in this notebook\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "import pandas as pd\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from textblob import TextBlob\n",
    "%matplotlib inline\n",
    "\n",
    "from string import punctuation\n",
    "from tqdm import tqdm\n",
    "\n",
    "import gensim\n",
    "from gensim import models\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import utils\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src = \"https://github.com/Akesari12/Computational-Social-Science-Training-Program/blob/master/images/cfpb%20logo.png?raw=true\"  />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CFPB dataset is a collection of complaints filed by consumers against financial institutions. The dataset contains a number of features, including the text of the complaint, the product type, and the company the complaint was filed against. The full dataset is available at https://www.consumerfinance.gov/data-research/consumer-complaints/. This dataset is convenient for text analysis because the consumer complaints are real text generated by real people - and have all the idiosyncrasies that come with that process. It also contains multiple different categories that we can predict, like type of product the complaint is about and whether the complaint was resolved quickly. The basic process is that if someone has a dispute related to consumer finance (mortgages, student loans, credit cards, etc.), they can file a dispute with the CFPB, which then contacts the company named in the dispute to get some resolution of the issue.\n",
    "\n",
    "We will use this dataset to explore the entire text analysis and machine learning pipeline. Specifically we will:\n",
    "\n",
    "1. Represent text as tokens and matrices to prepare them for machine learning\n",
    "2. Engineer additional text features like topic codes and sentiments with unsupervised learning techniques\n",
    "3. Use Supervised Learning to predict the \"Product\" category based on the text\n",
    "\n",
    "Notice how this notebook is full of comments. Much of the code here was generated with Github CoPilot! If you enjoy using ChatGPT to generate code, consider integrating it directly into your notebooks with this tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Date received</th>\n",
       "      <th>Product</th>\n",
       "      <th>Sub-product</th>\n",
       "      <th>Issue</th>\n",
       "      <th>Sub-issue</th>\n",
       "      <th>Consumer complaint narrative</th>\n",
       "      <th>Company public response</th>\n",
       "      <th>Company</th>\n",
       "      <th>State</th>\n",
       "      <th>ZIP code</th>\n",
       "      <th>Tags</th>\n",
       "      <th>Consumer consent provided?</th>\n",
       "      <th>Submitted via</th>\n",
       "      <th>Date sent to company</th>\n",
       "      <th>Company response to consumer</th>\n",
       "      <th>Timely response?</th>\n",
       "      <th>Consumer disputed?</th>\n",
       "      <th>Complaint ID</th>\n",
       "      <th>year_received</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2023-07-22</td>\n",
       "      <td>Credit card or prepaid card</td>\n",
       "      <td>General-purpose credit card or charge card</td>\n",
       "      <td>Problem with a purchase shown on your statement</td>\n",
       "      <td>Credit card company isn't resolving a dispute ...</td>\n",
       "      <td>Money is being held with out my permission. Ca...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Chime Financial Inc</td>\n",
       "      <td>CA</td>\n",
       "      <td>95678</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Consent provided</td>\n",
       "      <td>Web</td>\n",
       "      <td>2023-07-22</td>\n",
       "      <td>Closed with explanation</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7291298</td>\n",
       "      <td>2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2023-07-22</td>\n",
       "      <td>Credit reporting, credit repair services, or o...</td>\n",
       "      <td>Credit reporting</td>\n",
       "      <td>Improper use of your report</td>\n",
       "      <td>Credit inquiries on your report that you don't...</td>\n",
       "      <td>I do not have any knowledge of applying for a ...</td>\n",
       "      <td>Company has responded to the consumer and the ...</td>\n",
       "      <td>SYNCHRONY FINANCIAL</td>\n",
       "      <td>AZ</td>\n",
       "      <td>85741</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Consent provided</td>\n",
       "      <td>Web</td>\n",
       "      <td>2023-07-22</td>\n",
       "      <td>Closed with non-monetary relief</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7291403</td>\n",
       "      <td>2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2023-08-29</td>\n",
       "      <td>Credit reporting or other personal consumer re...</td>\n",
       "      <td>Credit reporting</td>\n",
       "      <td>Incorrect information on your report</td>\n",
       "      <td>Information belongs to someone else</td>\n",
       "      <td>Seriously, it's been months since I investigat...</td>\n",
       "      <td>Company has responded to the consumer and the ...</td>\n",
       "      <td>TRANSUNION INTERMEDIATE HOLDINGS, INC.</td>\n",
       "      <td>NJ</td>\n",
       "      <td>07087</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Consent provided</td>\n",
       "      <td>Web</td>\n",
       "      <td>2023-08-29</td>\n",
       "      <td>Closed with non-monetary relief</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7460791</td>\n",
       "      <td>2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2023-08-29</td>\n",
       "      <td>Credit reporting or other personal consumer re...</td>\n",
       "      <td>Credit reporting</td>\n",
       "      <td>Problem with a company's investigation into an...</td>\n",
       "      <td>Investigation took more than 30 days</td>\n",
       "      <td>I have been trying to dispute incorrect and/or...</td>\n",
       "      <td>Company has responded to the consumer and the ...</td>\n",
       "      <td>TRANSUNION INTERMEDIATE HOLDINGS, INC.</td>\n",
       "      <td>AZ</td>\n",
       "      <td>85013</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Consent provided</td>\n",
       "      <td>Web</td>\n",
       "      <td>2023-08-29</td>\n",
       "      <td>Closed with non-monetary relief</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7463275</td>\n",
       "      <td>2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2023-07-17</td>\n",
       "      <td>Credit reporting, credit repair services, or o...</td>\n",
       "      <td>Credit reporting</td>\n",
       "      <td>Credit monitoring or identity theft protection...</td>\n",
       "      <td>Received unwanted marketing or advertising</td>\n",
       "      <td>It shows that a mortgage company pulled my cre...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EQUIFAX, INC.</td>\n",
       "      <td>NC</td>\n",
       "      <td>27292</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Consent provided</td>\n",
       "      <td>Web</td>\n",
       "      <td>2023-07-17</td>\n",
       "      <td>Closed with explanation</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7264416</td>\n",
       "      <td>2023</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 Date received  \\\n",
       "0           0    2023-07-22   \n",
       "1           1    2023-07-22   \n",
       "2           2    2023-08-29   \n",
       "3           3    2023-08-29   \n",
       "4           4    2023-07-17   \n",
       "\n",
       "                                             Product  \\\n",
       "0                        Credit card or prepaid card   \n",
       "1  Credit reporting, credit repair services, or o...   \n",
       "2  Credit reporting or other personal consumer re...   \n",
       "3  Credit reporting or other personal consumer re...   \n",
       "4  Credit reporting, credit repair services, or o...   \n",
       "\n",
       "                                  Sub-product  \\\n",
       "0  General-purpose credit card or charge card   \n",
       "1                            Credit reporting   \n",
       "2                            Credit reporting   \n",
       "3                            Credit reporting   \n",
       "4                            Credit reporting   \n",
       "\n",
       "                                               Issue  \\\n",
       "0    Problem with a purchase shown on your statement   \n",
       "1                        Improper use of your report   \n",
       "2               Incorrect information on your report   \n",
       "3  Problem with a company's investigation into an...   \n",
       "4  Credit monitoring or identity theft protection...   \n",
       "\n",
       "                                           Sub-issue  \\\n",
       "0  Credit card company isn't resolving a dispute ...   \n",
       "1  Credit inquiries on your report that you don't...   \n",
       "2                Information belongs to someone else   \n",
       "3               Investigation took more than 30 days   \n",
       "4         Received unwanted marketing or advertising   \n",
       "\n",
       "                        Consumer complaint narrative  \\\n",
       "0  Money is being held with out my permission. Ca...   \n",
       "1  I do not have any knowledge of applying for a ...   \n",
       "2  Seriously, it's been months since I investigat...   \n",
       "3  I have been trying to dispute incorrect and/or...   \n",
       "4  It shows that a mortgage company pulled my cre...   \n",
       "\n",
       "                             Company public response  \\\n",
       "0                                                NaN   \n",
       "1  Company has responded to the consumer and the ...   \n",
       "2  Company has responded to the consumer and the ...   \n",
       "3  Company has responded to the consumer and the ...   \n",
       "4                                                NaN   \n",
       "\n",
       "                                  Company State ZIP code Tags  \\\n",
       "0                     Chime Financial Inc    CA    95678  NaN   \n",
       "1                     SYNCHRONY FINANCIAL    AZ    85741  NaN   \n",
       "2  TRANSUNION INTERMEDIATE HOLDINGS, INC.    NJ    07087  NaN   \n",
       "3  TRANSUNION INTERMEDIATE HOLDINGS, INC.    AZ    85013  NaN   \n",
       "4                           EQUIFAX, INC.    NC    27292  NaN   \n",
       "\n",
       "  Consumer consent provided? Submitted via Date sent to company  \\\n",
       "0           Consent provided           Web           2023-07-22   \n",
       "1           Consent provided           Web           2023-07-22   \n",
       "2           Consent provided           Web           2023-08-29   \n",
       "3           Consent provided           Web           2023-08-29   \n",
       "4           Consent provided           Web           2023-07-17   \n",
       "\n",
       "      Company response to consumer Timely response?  Consumer disputed?  \\\n",
       "0          Closed with explanation              Yes                 NaN   \n",
       "1  Closed with non-monetary relief              Yes                 NaN   \n",
       "2  Closed with non-monetary relief              Yes                 NaN   \n",
       "3  Closed with non-monetary relief              Yes                 NaN   \n",
       "4          Closed with explanation              Yes                 NaN   \n",
       "\n",
       "   Complaint ID  year_received  \n",
       "0       7291298           2023  \n",
       "1       7291403           2023  \n",
       "2       7460791           2023  \n",
       "3       7463275           2023  \n",
       "4       7264416           2023  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## load 2023 complaints data from data folder as CFPB_2023\n",
    "cfpb = pd.read_csv('./data/CFPB Complaints 2023.csv')\n",
    "#url = 'https://github.com/Akesari12/mpi_summer_school/raw/main/data/CFPB%20Complaints%202023.csv'\n",
    "\n",
    "#cfpb = pd.read_csv(url)\n",
    "\n",
    "# Drop observations with missing data in the 'Consumer complaint narrative' column and reset index\n",
    "cfpb = cfpb.dropna(subset=['Consumer complaint narrative']).reset_index(drop=True)\n",
    "\n",
    "# look at the first 5 rows of cfpb\n",
    "cfpb.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check out some descriptive statistics for this dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product</th>\n",
       "      <th>Issue</th>\n",
       "      <th>Company response to consumer</th>\n",
       "      <th>Timely response?</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>298530</td>\n",
       "      <td>298530</td>\n",
       "      <td>298530</td>\n",
       "      <td>298530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>14</td>\n",
       "      <td>85</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Credit reporting, credit repair services, or o...</td>\n",
       "      <td>Incorrect information on your report</td>\n",
       "      <td>Closed with explanation</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>211354</td>\n",
       "      <td>84770</td>\n",
       "      <td>185752</td>\n",
       "      <td>296878</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Product  \\\n",
       "count                                              298530   \n",
       "unique                                                 14   \n",
       "top     Credit reporting, credit repair services, or o...   \n",
       "freq                                               211354   \n",
       "\n",
       "                                       Issue Company response to consumer  \\\n",
       "count                                 298530                       298530   \n",
       "unique                                    85                            5   \n",
       "top     Incorrect information on your report      Closed with explanation   \n",
       "freq                                   84770                       185752   \n",
       "\n",
       "       Timely response?  \n",
       "count            298530  \n",
       "unique                2  \n",
       "top                 Yes  \n",
       "freq             296878  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show some descriptive statistics for the product, issue, company response to consumer, timely response, and consumer disputed columns, but ignore NAs in each column\n",
    "cfpb[['Product', 'Issue', 'Company response to consumer', 'Timely response?']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's see what some of these complaints look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Money is being held with out my permission. Causing me to have pain and suffering for me and my daughter. Horrible customer service and rude employees lie about the policy and does not follow it. I am having a horrible experience with chime and it needs to be resolved.\n",
      "1 I do not have any knowledge of applying for a credit card with XXXX. Nor do I have a contract with the debt collector. I demand this inquiry be removed from my credit report.\n",
      "2 Seriously, it's been months since I investigated my credit report, and I realized that some of the information was still erroneous. The 3 credit bureaus are required to authenticate these items under Sections 609 ( a ) ( 1 ) ( A ) and 611 ( a ) ( 1 ). ( A ). It is not acceptable to treat these reporting items as unconfirmed information without producing proof within the legal time range. Sections 609 ( a ) ( 1 ) ( A ) and 611 ( a ) ( 1 ). ( A ), Please investigate the unverifiable items and remove them immediately from my report XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX XXXX\n",
      "3 I have been trying to dispute incorrect and/or invalid items listed on my credit report and have gotten no assistance from the creditors or the credit bureaus regarding this matter. Please see attached documents and letter for referenced issues from above.\n",
      "4 It shows that a mortgage company pulled my credit report on XX/XX/2023, however I did not apply for a mortgage.\n",
      "5 I was scammed using XXXX from a bank I trust. The scammer is not enrolled with XXXX and XXXX even sent me a text message stating my transaction did not go through to try again so like a dummy I did I got the same message. I was told both time and even by my bank the recipient was not enrolled with XXXX so I should get a refund right? Nope USAA contracted the recipient bank who refused to give me a refund. I have sent USAA proof that shows I should get my money back. I have called several times and wrote them giving them documents of proof this was a month ago and still no reply from them. I have enclosed document # 14 clearly shows that the money taken from my checking account and I think given to XXXX went where since it states recipient is not enrolled with XXXX. # 16 states also recipient is not registered with XXXX this is the email she told me to send the money with XXXX her name XXXX XXXX and email XXXX using XXXX XXXX suddenly accounts are not enrolled and unregistered if so where did my money go to? My bank should be listening to me I gave them pages of proof. Please help! By the way I did not receive my tickets and they are still trying to sell them on XXXX sites.\n",
      "6 I AM SUBMITTING THIS WITHOUT ANY INFLUENCE AND THIS IS NOT A THIRD PARTY. THE CREDIT BUREAUS STATED MY ACC WAS PROPERLY INVESTIGATED BUT HOW IS THAT POSSIBLE IF THE OPEN DATE IS INACCURATE, THE DATE LAST ACTIVE IS INACCURATE, AND THE DATE LAST REPORTED IS NOT ACCURATE? THIS GROUND FOR REMOVAL, THEY ALSO VIOLATED MY RIGHTS UNDER 15 U.S.C 1681 SECTION 602 A. STATES I HAVE THE RIGHT TO PRIVACY.\n",
      "\n",
      "15 U.S.C 1681 SECTION 604 SECTION 2. IT ALSO STATES A CONSUMER REPORTING AGENCY CAN NOT FURNISH AN ACCOUNT WITHOUT MY WRITTEN INSTRUCTIONS. \n",
      "\n",
      "PLEASE SEE ATTACHED LETTERS.\n",
      "7 This account does not belong to me the social security number does not belongs to me and is being reported to XXXX in which I have contacted TD Banks about accounts this account starts or ends with XXXX and XXXX and this being reported illegally to my name in which this account is not mine\n",
      "8 I was defrauded by an auto mechanic in XXXX and I immediately reached out to Bank of America and Visa, the card issuer, to assist with consumer protection. I brought my car in for a FULL SYNTHETIC OIL CHANGE AND COMPLETE TUNE UP, but the mechanic performed a conventional oil change and only an incomplete tune up. When I collected my vehicle, I noticed on the receipt that this was not what had been discussed in person and by phone with the shop manager. The clerk informed me that management was away and I would have to call. I did, immediately, and spoke with tech, who apologetically conceded that they did not perform the agreed to repairs. He provided contact information and informed me that I could also expect to receive a call from his manager. When reached by phone, she similarly apologized and offered a partial refund and a re-do of some of the work. I agreed, but when I brought the vehicle back to the shop, the aging owner aggressively approached me. He overruled her on the refund and re-work and berated me in front of the customers. He invaded my personal space and I left the store with a warning that, had I stayed, I would need to involve the police because he was becoming belligerent. About 5-10 minutes, later I received a frantic call from the manager again asking me to return to the store for a partial refund. I informed her that I did not feel safe returning to the store and she admitted that he was out of control. I told her I would handle it with my card company and bank. The Bank of America representative assured me that I neednt worry and that my Visa/BOA card was fraud-protected. The man did not perform the work that was agreed to and I should be due for a full refund. I contacted the Better Business Bureau, but the shop did not cooperate. I similarly reached out to an association of auto mechanics. I eventually received a temporary credit for the charge. I thought this was over until Thursday, when, without warning, I saw that the amount of {$460.00} was withdrawn from my account. I had not even been consulted. Had they asked, I could have included further testimony and documentation. This man, whos shop is currently 1-star rated, did not perform the work he was paid to complete and then physically threatened me when I attempted to resolve the problem. He did perform some work, but it was not what was paid for. I was completely defrauded and the card company inexplicably sided with a merchant with a history of intimidation and shoddy workmanship. I am flabbergasted and have since reached out to local news media about both the mechanic shop and Visas response.\n",
      "9 Chase bank continues to place unnecessary holds on large deposits into my bank account. For example, my wife wrote a check to me for {$30000.00}. I deposited the check into my Chase bank account on XX/XX/23. The funds were withdrawn from my wife 's XXXX XXXX XXXX account on XX/XX/23. As of today ( XX/XX/23 ), Chase still has a hold on the funds in my account. The following message is displaying in my account online : \" We need more time to collect the money from the paying bank so your deposited funds aren't available yet. '' This is 100 % not true. There is no reason why the funds should still be on hold in my bank account when the funds were withdrawn from my wife 's account 2 days ago. If the funds are not in my wife 's bank account and they are not in my bank account... where are the funds???? I need these funds asap.\n"
     ]
    }
   ],
   "source": [
    "# Check the first 10 rows of the consumer complaint narrative column and print the full text of each with a number next to each complaint\n",
    "for i in range(10):\n",
    "    print(i, cfpb['Consumer complaint narrative'][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our supervised machine learning task, let's simplify to just two categories (debt collection and credit card issues). If there is time permitting at the end, we can explore other categories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Credit card or prepaid card',\n",
       "       'Credit reporting, credit repair services, or other personal consumer reports',\n",
       "       'Credit reporting or other personal consumer reports',\n",
       "       'Checking or savings account', 'Debt collection',\n",
       "       'Money transfer, virtual currency, or money service',\n",
       "       'Vehicle loan or lease', 'Credit card', 'Student loan', 'Mortgage',\n",
       "       'Prepaid card', 'Payday loan, title loan, or personal loan',\n",
       "       'Payday loan, title loan, personal loan, or advance loan',\n",
       "       'Debt or credit management'], dtype=object)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfpb['Product'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Debt collection                22309\n",
       "Credit card or prepaid card    19948\n",
       "Name: Product, dtype: int64"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Subset cfpb so that Product is one of 'Debt Collection' or \"Credit Card or Prepaid Card\"\n",
    "\n",
    "cfpb = cfpb[cfpb['Product'].isin(['Debt collection', 'Credit card or prepaid card'])].reset_index(drop=True)\n",
    "cfpb['Product'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src = \"https://github.com/Akesari12/CELS-ML-Text-Analysis-Workshop/blob/main/images/ml_workflow.png?raw=true\"  />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The typical machine learning workflow involves data collection, processing, training models, testing models, and deploying models. We took care of the data collection part, and now will focus on some techniques for cleaning, training, and evaluating machine learning models for text analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Text Featurization Using spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text needs to be represented numerically before it can be used in a machine learning pipeline. We will cover some of the basic building blocks for:\n",
    "\n",
    "1. Representing text as \"tokens\"\n",
    "2. Creating matrices that count and scale how often words occur in a corpus of text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Stemming, Lemmatization, and Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.1 Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic unit of analysis in text analysis is the **token**, which is a numerical representation of text. Imagine we are trying to build a puzzle. Each \"token\" represents a piece of the puzzle, and can correspond to different sizes (like paragraphs, sentences, or words). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how we tokenize in practice. spaCy uses pre-trained language models to incorporate context when creating tokens. In this case, we'll load the [en_core_web_sm](https://spacy.io/models/en), which is one of spaCy's English language models. \n",
    "\n",
    "Consider the sentence: \"Last year, the average cost of a coffee cup in the U.S.A was $5.89.\" Splitting on punctuation would yield a token list like:\n",
    "\n",
    "[Last, year, the, average, cost, of, a, coffee, cup, in, the, U, S, A, was, $5, 89]\n",
    "\n",
    "Most of these tokens look pretty good, but we get some errors like splitting \"USA\" into three separate tokens and spltting \\$5.89 into \\\\$5 and 89. \n",
    "\n",
    "spaCy helps [solve this problem](https://spacy.io/usage/spacy-101#annotations-token) by going through the following steps:\n",
    "\n",
    "First, the raw text is split on whitespace characters, similar to `text.split(' ')`. Then, the tokenizer processes the text from left to right. On each substring, it performs two checks:\n",
    "\n",
    "1. Does the substring match a tokenizer exception rule? For example, “don’t” does not contain whitespace, but should be split into two tokens, “do” and “n’t”, while “U.S.A” should always remain one token.\n",
    "2. Can a prefix, suffix or infix be split off? For example punctuation like commas, periods, hyphens or quotes.\n",
    "\n",
    "If there’s a match, the rule is applied and the tokenizer continues its loop, starting with the newly split substrings. This way, spaCy can split complex, nested tokens like combinations of abbreviations and multiple punctuation marks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Last year, the average cost of a coffee cup in the U.S.A was ', '5.89.']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Last year, the average cost of a coffee cup in the U.S.A was $5.89.\".split('$') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try applying these methods to our CFPB data. The steps to do this are:\n",
    "\n",
    "1. Load the language model.\n",
    "2. Apply it to a piece of text and save it in an spaCy \"doc\" object.\n",
    "3. Extract each token from the doc object to a list.\n",
    "4. Display the tokens\n",
    "\n",
    "Let's do this for our first observation in our dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Tokenized words: ['Money', 'is', 'being', 'held', 'with', 'out', 'my', 'permission', '.', 'Causing', 'me', 'to', 'have', 'pain', 'and', 'suffering', 'for', 'me', 'and', 'my', 'daughter', '.', 'Horrible', 'customer', 'service', 'and', 'rude', 'employees', 'lie', 'about', 'the', 'policy', 'and', 'does', 'not', 'follow', 'it', '.', 'I', 'am', 'having', 'a', 'horrible', 'experience', 'with', 'chime', 'and', 'it', 'needs', 'to', 'be', 'resolved', '.']\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# save the first cfpb consumer complaint as \"text\"\n",
    "text = cfpb['Consumer complaint narrative'][0]\n",
    "\n",
    "# load the spaCy language model\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "# Apply it to a piece of text and save it in a spaCy \"doc\" object\n",
    "doc = nlp(text)\n",
    "\n",
    "# Extract each token from the doc object to a list\n",
    "spacy_words = [token.text for token in doc]\n",
    "\n",
    "# Display the tokens\n",
    "display(f\"Tokenized words: {spacy_words}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2 Removing Stop Words and Punctuation\n",
    "\n",
    "We now have some tokens with just a few lines of code! There are a few additional steps that we might want to take. For example, we may want to remove punctuation and stop words. Punctuation oftentimes does not add substantive information to a piece of text, and stop words are common words that appear very frequently across texts. Removing this kind of information can help with downstream classification tasks by allowing an algorithm to focus on words that distinguish documents, rather than ones that appear frequently across them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's take a look at stop words. We can start by importing a collection of stop words from spaCy by running the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['re', 'does', 'even', 'among', 'your', 'during', 'hereupon', 'four', 'further', 'show']\n"
     ]
    }
   ],
   "source": [
    "# import stop words and look at the 10 most common ones\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "print(list(STOP_WORDS)[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do the same for punctuation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "# import punctuation\n",
    "from string import punctuation\n",
    "print(punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing punctuation and stop words is not a hard and fast rule - there may be situations where you want to keep them. In most applications, they add noise to downstream tasks, but always be mindful of your particular application when making decisions. Now that we have some tokenization tools, let's put them all together in a function!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Challenge: Write a function that takes a piece of text as an argument, and returns a list of tokens without punctuation or stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Money',\n",
       " 'held',\n",
       " 'permission',\n",
       " 'Causing',\n",
       " 'pain',\n",
       " 'suffering',\n",
       " 'daughter',\n",
       " 'Horrible',\n",
       " 'customer',\n",
       " 'service']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write a function, with comments, `rem_punc_stop` that takes the argument `text` and returns a list of tokens that are free of punctuation and stop words.\n",
    "def rem_punc_stop(text):\n",
    "    # create a spaCy doc object\n",
    "    doc = nlp(text)\n",
    "    # create a list of tokens from the doc object\n",
    "    tokens = [token.text for token in doc]\n",
    "    # create a list of tokens that are not stop words or punctuation\n",
    "    tokens_reduced = [token for token in tokens if (token not in STOP_WORDS) and (token not in punctuation)]\n",
    "    # return the list of tokens\n",
    "    return tokens_reduced\n",
    "\n",
    "\n",
    "# test rem_punc_stop with the text from the first cfpb complaint and save the result in tokens_reduced\n",
    "tokens_reduced = rem_punc_stop(text)\n",
    "tokens_reduced[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spaCy also contains a number of methods for things like entity recognition. For instance, we could run the following code to check various entities. Notice that this process isn't perfect, spaCy still thinks \"XX/XX/XXXX\" is an organization or product even though we know this is a redacted date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "for entity in nlp(text).ents:\n",
    "    print(entity.text + ' - ' + entity.label_ + ' - ' + str(spacy.explain(entity.label_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.3 Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another preprocessing step we might take is reducing words down to their lemmas. Lemmatization reduces a word to its root word, while making sure the word still belongs to the language. Consider the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compute compute\n",
      "computer computer\n",
      "computed compute\n",
      "computing computing\n"
     ]
    }
   ],
   "source": [
    "# write code that goes through the string 'compute computer computed computing' and prints the text and lemma\n",
    "doc = nlp('compute computer computed computing')\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming these words would all result in the root \"comput\" but lemmatization converted these words to their shortest variant. Again, you may choose to stem or lemmatize depending on your specific application. Let's see what some of our lemmas look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Money money\n",
      "is be\n",
      "being be\n",
      "held hold\n",
      "with with\n",
      "out out\n",
      "my my\n",
      "permission permission\n",
      ". .\n",
      "Causing cause\n",
      "me I\n",
      "to to\n",
      "have have\n",
      "pain pain\n",
      "and and\n",
      "suffering suffer\n",
      "for for\n",
      "me I\n",
      "and and\n",
      "my my\n",
      "daughter daughter\n",
      ". .\n",
      "Horrible horrible\n",
      "customer customer\n",
      "service service\n",
      "and and\n",
      "rude rude\n",
      "employees employee\n",
      "lie lie\n",
      "about about\n",
      "the the\n",
      "policy policy\n",
      "and and\n",
      "does do\n",
      "not not\n",
      "follow follow\n",
      "it it\n",
      ". .\n",
      "I I\n",
      "am be\n",
      "having have\n",
      "a a\n",
      "horrible horrible\n",
      "experience experience\n",
      "with with\n",
      "chime chime\n",
      "and and\n",
      "it it\n",
      "needs need\n",
      "to to\n",
      "be be\n",
      "resolved resolve\n",
      ". .\n"
     ]
    }
   ],
   "source": [
    "for word in nlp(text):\n",
    "    print(word.text,  word.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.3 Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another preprocessing step we might take is reducing words down to their lemmas. Lemmatization reduces a word to its root word, while making sure the word still belongs to the language. Consider the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compute compute\n",
      "computer computer\n",
      "computed compute\n",
      "computing computing\n"
     ]
    }
   ],
   "source": [
    "# write code that goes through the string 'compute computer computed computing' and prints the text and lemma\n",
    "doc = nlp('compute computer computed computing')\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words/Tf-idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we go from tokens to making predictions with text? Supervised machine learning applications will generally have a `target` to be predicted and `features` used to make that prediction. These are the equivalents of the `dependent variable`/`response variable`/`outcome` and `independent variables`/`explanatory variables`/`covariates` in machine learning terms. There are many options for converting raw text into `features`. We will start with **bag of words**, one of the simplest ways to represent tokens as counts. Consider the following toy example:\n",
    "\n",
    "| Product                        | money | harass | social | Visa | credit |\n",
    "|--------------------------------|-------|--------|--------|------|--------|\n",
    "| Debt or Debt Collection        | 5     | 7      | 2      | 0    | 4      |\n",
    "| Credit card or prepaid card    | 3     | 1      | 3      | 5    | 6      |\n",
    "| Credit card or prepaid card    | 4     | 0      | 1      | 0    | 3      |\n",
    "| Debt or Debt Collection        | 2     | 1      | 0      | 0    | 2      |\n",
    "| Debt or Debt Collection        | 1     | 2      | 3      | 0    | 1      |\n",
    "\n",
    "\n",
    "In this data, each row is a separate consumer complaint. *Product* is the target we want to predict, and each column represents a token. Each cell represents how many times that token appears in that document.\n",
    "\n",
    "Let's see how we do this with code. We'll import the CounterVectorizer method from sklearn, then use the tokenizer function that we wrote earlier to initialize the CountVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize CounterVectorizer then use rem_punc_stop as the tokenizer and save as bow_vector\n",
    "bow_vector = CountVectorizer(tokenizer=rem_punc_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we create a CountVectorizer object, we can then transform a list of texts with the \"fit_transform\" method. This will return a sparse matrix with the counts. We can densify the matrix (store non-zero values and their positions) with the \".todense()\" method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Anike\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# fit bow_vector to the first 5 complaints in cfpb['Consumer complaint narrative'] and save as bow_matrix\n",
    "bow_matrix = bow_vector.fit_transform(cfpb['Consumer complaint narrative'][0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 0,  0,  0, ...,  0,  0,  0],\n",
       "        [ 0,  0,  0, ...,  1,  0,  0],\n",
       "        [ 0,  0,  3, ..., 13,  1,  1],\n",
       "        [ 0,  1,  0, ...,  3,  0,  0],\n",
       "        [ 1,  0,  0, ..., 10,  0,  0]], dtype=int64)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# densify bow_matrix and save as bow_matrix_dense\n",
    "bow_matrix_dense = bow_matrix.todense()\n",
    "bow_matrix_dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also get the feature names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['\\n', '\\n\\n', ' ', '1', '10', '100', '2300.00', '460.00', '5',\n",
       "       '890.00', 'account', 'additional', 'admitted', 'advise',\n",
       "       'affected', 'agency', 'aggressively', 'aging', 'agreed', 'america',\n",
       "       'apologetically', 'apologized', 'approached', 'asked', 'asking',\n",
       "       'assist', 'association', 'assured', 'attempted', 'auto', 'away',\n",
       "       'balance', 'bank', 'behalf', 'believe', 'belligerent', 'berated',\n",
       "       'better', 'bill', 'boa', 'brought', 'bureau', 'business', 'called',\n",
       "       'car', 'card', 'causing', 'certainly', 'change', 'charge'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the feature names from bow_vector and print the first 50\n",
    "feature_names = bow_vector.get_feature_names_out()\n",
    "feature_names[0:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term Frequency - Inverse Document Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An extension of bag-of-words is the term frequency-inverse document frequency approach. Whereas bag-of-words counts the number of words in the document. tf-idf takes this quanity and divides it by how frequently the word shows up across the corpus. In notation:\n",
    "\n",
    "$TF-IDF(t,d)=TF(t,d)×IDF(t)$\n",
    "\n",
    "Where:\n",
    "\n",
    "* $TF(t,d)$ is term frequency of term $t$ in document $d$ (number of times a token appears in that consumer complaint)\n",
    "* $IDF(f) = log(\\frac{N}{DF(t)}$ is the inverse document frequency (where $N$ is number of documents, $DF(t)$ is the number of documents containing term $t$). (Log of number of consumer complaints/number of consumer complaints containing a token)\n",
    "\n",
    "In doing so, the tf-idf score downweights words that are common in the corpus and thus would not aid with classification. Our toy example from before would look like this:\n",
    "\n",
    "| Product      | money |   harass |   social |     Visa | credit |\n",
    "|-------|-------|----------|----------|----------|--------|\n",
    "| Debt or Debt Collection |   0.0 | 1.562005 | 0.446287 |  0.00000 |    0.0 |\n",
    "| Credit card or prepaid card |   0.0 | 0.223144 | 0.669431 |  8.04719 |    0.0 |\n",
    "| Credit card or prepaid card |   0.0 | 0.000000 | 0.223144 |  0.00000 |    0.0 |\n",
    "| Debt or Debt Collection |   0.0 | 0.223144 | 0.000000 |  0.00000 |    0.0 |\n",
    "| Debt or Debt Collection |   0.0 | 0.446287 | 0.669431 |  0.00000 |    0.0 |\n",
    "\n",
    "\n",
    "**Challenge: Using the code from the \"Bag of Words\" section as a template, write code to get the tf-idf matrix for the CFPB data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>\\n</th>\n",
       "      <th>\\n\\n</th>\n",
       "      <th>\\n\\n\\n</th>\n",
       "      <th>\\n\\n\\n\\n</th>\n",
       "      <th>\\n\\n\\n\\n\\n</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>''</th>\n",
       "      <th>...</th>\n",
       "      <th>.they</th>\n",
       "      <th>.yes</th>\n",
       "      <th>//consumer.ftc.gov</th>\n",
       "      <th>//consumer.georgia.gov</th>\n",
       "      <th>//portal.mrsbpo.com</th>\n",
       "      <th>//www.capitalone.com</th>\n",
       "      <th>//www.capitalone.comxxxx</th>\n",
       "      <th>//www.consumerfinance.gov</th>\n",
       "      <th>//www.credencerm.com</th>\n",
       "      <th>/credit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.127439</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.060741</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.087553</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.050340</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.116789</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.055968</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.142590</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            \\n      \\n\\n  \\n\\n\\n  \\n\\n\\n\\n  \\n\\n\\n\\n\\n                 \\\n",
       "0     0.000000  0.000000     0.0       0.0         0.0  0.000000  0.0   \n",
       "1     0.000000  0.000000     0.0       0.0         0.0  0.000000  0.0   \n",
       "2     0.000000  0.000000     0.0       0.0         0.0  0.127439  0.0   \n",
       "3     0.000000  0.060741     0.0       0.0         0.0  0.000000  0.0   \n",
       "4     0.087553  0.000000     0.0       0.0         0.0  0.000000  0.0   \n",
       "...        ...       ...     ...       ...         ...       ...  ...   \n",
       "1995  0.000000  0.000000     0.0       0.0         0.0  0.050340  0.0   \n",
       "1996  0.000000  0.116789     0.0       0.0         0.0  0.000000  0.0   \n",
       "1997  0.000000  0.055968     0.0       0.0         0.0  0.000000  0.0   \n",
       "1998  0.000000  0.000000     0.0       0.0         0.0  0.000000  0.0   \n",
       "1999  0.000000  0.000000     0.0       0.0         0.0  0.142590  0.0   \n",
       "\n",
       "                                                  ''  ...  .they  .yes  \\\n",
       "0         0.0                               0.0  0.0  ...    0.0   0.0   \n",
       "1         0.0                               0.0  0.0  ...    0.0   0.0   \n",
       "2         0.0                               0.0  0.0  ...    0.0   0.0   \n",
       "3         0.0                               0.0  0.0  ...    0.0   0.0   \n",
       "4         0.0                               0.0  0.0  ...    0.0   0.0   \n",
       "...       ...                               ...  ...  ...    ...   ...   \n",
       "1995      0.0                               0.0  0.0  ...    0.0   0.0   \n",
       "1996      0.0                               0.0  0.0  ...    0.0   0.0   \n",
       "1997      0.0                               0.0  0.0  ...    0.0   0.0   \n",
       "1998      0.0                               0.0  0.0  ...    0.0   0.0   \n",
       "1999      0.0                               0.0  0.0  ...    0.0   0.0   \n",
       "\n",
       "      //consumer.ftc.gov  //consumer.georgia.gov  //portal.mrsbpo.com  \\\n",
       "0                    0.0                     0.0                  0.0   \n",
       "1                    0.0                     0.0                  0.0   \n",
       "2                    0.0                     0.0                  0.0   \n",
       "3                    0.0                     0.0                  0.0   \n",
       "4                    0.0                     0.0                  0.0   \n",
       "...                  ...                     ...                  ...   \n",
       "1995                 0.0                     0.0                  0.0   \n",
       "1996                 0.0                     0.0                  0.0   \n",
       "1997                 0.0                     0.0                  0.0   \n",
       "1998                 0.0                     0.0                  0.0   \n",
       "1999                 0.0                     0.0                  0.0   \n",
       "\n",
       "      //www.capitalone.com  //www.capitalone.comxxxx  \\\n",
       "0                      0.0                       0.0   \n",
       "1                      0.0                       0.0   \n",
       "2                      0.0                       0.0   \n",
       "3                      0.0                       0.0   \n",
       "4                      0.0                       0.0   \n",
       "...                    ...                       ...   \n",
       "1995                   0.0                       0.0   \n",
       "1996                   0.0                       0.0   \n",
       "1997                   0.0                       0.0   \n",
       "1998                   0.0                       0.0   \n",
       "1999                   0.0                       0.0   \n",
       "\n",
       "      //www.consumerfinance.gov  //www.credencerm.com  /credit  \n",
       "0                           0.0                   0.0      0.0  \n",
       "1                           0.0                   0.0      0.0  \n",
       "2                           0.0                   0.0      0.0  \n",
       "3                           0.0                   0.0      0.0  \n",
       "4                           0.0                   0.0      0.0  \n",
       "...                         ...                   ...      ...  \n",
       "1995                        0.0                   0.0      0.0  \n",
       "1996                        0.0                   0.0      0.0  \n",
       "1997                        0.0                   0.0      0.0  \n",
       "1998                        0.0                   0.0      0.0  \n",
       "1999                        0.0                   0.0      0.0  \n",
       "\n",
       "[2000 rows x 50 columns]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize TfidfVectorizer then use rem_punc_stop as the tokenizer and save as tfidf_vector\n",
    "tfidf_vector = TfidfVectorizer(tokenizer=rem_punc_stop)\n",
    "\n",
    "# fit tfidf_vector to the first 5 complaints in cfpb['Consumer complaint narrative'] and save as tfidf_matrix\n",
    "tfidf_matrix = tfidf_vector.fit_transform(cfpb['Consumer complaint narrative'][0:2000])\n",
    "\n",
    "# get the feature names from tfidf_vector, make a dataframe with the features names and tf-idf counts, and show the first 50 columns\n",
    "feature_names = tfidf_vector.get_feature_names_out()\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.todense(), columns=feature_names)\n",
    "tfidf_df.iloc[:, 0:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we want features that capture more information than just word counts? We will explore a few options for doing this:\n",
    "\n",
    "1. Topic Models\n",
    "2. Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine learning can be divided into `supervised learning` or `unsupervised learning`. Supervised learning uses labeled data to map inputs to outputs (e.g. predict a Product category based on features in the CFPB dataset), whereas unsupervised learning finds patterns in unlabeled data (e.g. finds geographic clusters of \"medical debt\" complaints in the CFPB dataset).\n",
    "\n",
    "One popular unsupervised method for text is the [Latent Dirichlet Allocation (LDA)](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation), or topic modeling. Topic models can help us uncover structure within a text. Specifically it does so through a \"mixture model\" - meaning every document is assumed to be \"about\" various topics, and we try to estimate the proportion each topic contributes to a document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply a LDA model. The key hyperparameter here is the `n_components` argument. Let's start with 5, and then print out our topics to see what we get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TOPICS = 5 # choose the number of topics\n",
    "\n",
    "# initialize LatentDirichletAllocation with 5 topics, 20 iterations, and a random state of 0, then fit it to tfidf_matrix\n",
    "lda = LatentDirichletAllocation(n_components=N_TOPICS, max_iter=20, random_state=0)\n",
    "lda = lda.fit(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "late account accounts update documents start attached reporting bureau want\n",
      "Topic 1:\n",
      "verizon zions repossession unconstitutional misstatements recognized gemini onxx unlawful pfcu\n",
      "Topic 2:\n",
      "sequium 3x capio radius global phones 6900.00 boyfriend weinberg weltman\n",
      "Topic 3:\n",
      "xxxx xx credit debt account card company \n",
      "\n",
      " bank report\n",
      "Topic 4:\n",
      "2001 incidents flied amercollect hoa ben clam afni thifet guitar\n"
     ]
    }
   ],
   "source": [
    "# print the top 10 words for each topic\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    print(f\"Topic {topic_idx}:\")\n",
    "    print(\" \".join([feature_names[i] for i in topic.argsort()[:-11:-1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic weights\n",
    "\n",
    "One thing we may want to do with the output is compare the prevalence of each topic across documents. A simple way to do this, is to merge the topic distribution back into the Pandas dataframe.\n",
    "\n",
    "First get the topic distribution array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic_0</th>\n",
       "      <th>topic_1</th>\n",
       "      <th>topic_2</th>\n",
       "      <th>topic_3</th>\n",
       "      <th>topic_4</th>\n",
       "      <th>index</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Date received</th>\n",
       "      <th>Product</th>\n",
       "      <th>Sub-product</th>\n",
       "      <th>...</th>\n",
       "      <th>ZIP code</th>\n",
       "      <th>Tags</th>\n",
       "      <th>Consumer consent provided?</th>\n",
       "      <th>Submitted via</th>\n",
       "      <th>Date sent to company</th>\n",
       "      <th>Company response to consumer</th>\n",
       "      <th>Timely response?</th>\n",
       "      <th>Consumer disputed?</th>\n",
       "      <th>Complaint ID</th>\n",
       "      <th>year_received</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.040545</td>\n",
       "      <td>0.038479</td>\n",
       "      <td>0.038479</td>\n",
       "      <td>0.844017</td>\n",
       "      <td>0.038480</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-07-22</td>\n",
       "      <td>Credit card or prepaid card</td>\n",
       "      <td>General-purpose credit card or charge card</td>\n",
       "      <td>...</td>\n",
       "      <td>95678</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Consent provided</td>\n",
       "      <td>Web</td>\n",
       "      <td>2023-07-22</td>\n",
       "      <td>Closed with explanation</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7291298</td>\n",
       "      <td>2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.018113</td>\n",
       "      <td>0.018035</td>\n",
       "      <td>0.018035</td>\n",
       "      <td>0.927781</td>\n",
       "      <td>0.018036</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>2023-08-12</td>\n",
       "      <td>Credit card or prepaid card</td>\n",
       "      <td>General-purpose credit card or charge card</td>\n",
       "      <td>...</td>\n",
       "      <td>33325</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Consent provided</td>\n",
       "      <td>Web</td>\n",
       "      <td>2023-08-12</td>\n",
       "      <td>Closed with explanation</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7389759</td>\n",
       "      <td>2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.026777</td>\n",
       "      <td>0.026395</td>\n",
       "      <td>0.026394</td>\n",
       "      <td>0.894039</td>\n",
       "      <td>0.026395</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>2023-07-20</td>\n",
       "      <td>Debt collection</td>\n",
       "      <td>Medical debt</td>\n",
       "      <td>...</td>\n",
       "      <td>498XX</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Consent provided</td>\n",
       "      <td>Web</td>\n",
       "      <td>2023-07-20</td>\n",
       "      <td>Closed with non-monetary relief</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7279944</td>\n",
       "      <td>2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.029520</td>\n",
       "      <td>0.029366</td>\n",
       "      <td>0.029365</td>\n",
       "      <td>0.882383</td>\n",
       "      <td>0.029366</td>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "      <td>2023-07-17</td>\n",
       "      <td>Debt collection</td>\n",
       "      <td>Credit card debt</td>\n",
       "      <td>...</td>\n",
       "      <td>67212</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Consent provided</td>\n",
       "      <td>Web</td>\n",
       "      <td>2023-07-17</td>\n",
       "      <td>Closed with explanation</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7261717</td>\n",
       "      <td>2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.222164</td>\n",
       "      <td>0.036218</td>\n",
       "      <td>0.036217</td>\n",
       "      <td>0.669184</td>\n",
       "      <td>0.036218</td>\n",
       "      <td>4</td>\n",
       "      <td>18</td>\n",
       "      <td>2023-07-21</td>\n",
       "      <td>Debt collection</td>\n",
       "      <td>Payday loan debt</td>\n",
       "      <td>...</td>\n",
       "      <td>30236</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Consent provided</td>\n",
       "      <td>Web</td>\n",
       "      <td>2023-07-21</td>\n",
       "      <td>Closed with explanation</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7286377</td>\n",
       "      <td>2023</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    topic_0   topic_1   topic_2   topic_3   topic_4  index  Unnamed: 0  \\\n",
       "0  0.040545  0.038479  0.038479  0.844017  0.038480      0           0   \n",
       "1  0.018113  0.018035  0.018035  0.927781  0.018036      1           8   \n",
       "2  0.026777  0.026395  0.026394  0.894039  0.026395      2          15   \n",
       "3  0.029520  0.029366  0.029365  0.882383  0.029366      3          17   \n",
       "4  0.222164  0.036218  0.036217  0.669184  0.036218      4          18   \n",
       "\n",
       "  Date received                      Product  \\\n",
       "0    2023-07-22  Credit card or prepaid card   \n",
       "1    2023-08-12  Credit card or prepaid card   \n",
       "2    2023-07-20              Debt collection   \n",
       "3    2023-07-17              Debt collection   \n",
       "4    2023-07-21              Debt collection   \n",
       "\n",
       "                                  Sub-product  ... ZIP code Tags  \\\n",
       "0  General-purpose credit card or charge card  ...    95678  NaN   \n",
       "1  General-purpose credit card or charge card  ...    33325  NaN   \n",
       "2                                Medical debt  ...    498XX  NaN   \n",
       "3                            Credit card debt  ...    67212  NaN   \n",
       "4                            Payday loan debt  ...    30236  NaN   \n",
       "\n",
       "  Consumer consent provided? Submitted via Date sent to company  \\\n",
       "0           Consent provided           Web           2023-07-22   \n",
       "1           Consent provided           Web           2023-08-12   \n",
       "2           Consent provided           Web           2023-07-20   \n",
       "3           Consent provided           Web           2023-07-17   \n",
       "4           Consent provided           Web           2023-07-21   \n",
       "\n",
       "      Company response to consumer Timely response? Consumer disputed?  \\\n",
       "0          Closed with explanation              Yes                NaN   \n",
       "1          Closed with explanation              Yes                NaN   \n",
       "2  Closed with non-monetary relief              Yes                NaN   \n",
       "3          Closed with explanation              Yes                NaN   \n",
       "4          Closed with explanation              Yes                NaN   \n",
       "\n",
       "  Complaint ID year_received  \n",
       "0      7291298          2023  \n",
       "1      7389759          2023  \n",
       "2      7279944          2023  \n",
       "3      7261717          2023  \n",
       "4      7286377          2023  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the distribution of each topic across documents by merging the topic distribution into the original cfpb dataframe and call it cfpb_topics\n",
    "topic_dist = lda.transform(tfidf_matrix)\n",
    "topic_dist_df = pd.DataFrame(topic_dist, columns=[f\"topic_{i}\" for i in range(N_TOPICS)])\n",
    "cfpb_topics = topic_dist_df.join(cfpb.reset_index())\n",
    "cfpb_topics.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can check the average weight of each topic across Product using `groupby`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product\n",
      "Credit card or prepaid card    0.266415\n",
      "Debt collection                0.163424\n",
      "Name: topic_0, dtype: float64\n",
      "Product\n",
      "Debt collection                0.038923\n",
      "Credit card or prepaid card    0.036989\n",
      "Name: topic_1, dtype: float64\n",
      "Product\n",
      "Debt collection                0.040771\n",
      "Credit card or prepaid card    0.038990\n",
      "Name: topic_2, dtype: float64\n",
      "Product\n",
      "Debt collection                0.720038\n",
      "Credit card or prepaid card    0.621046\n",
      "Name: topic_3, dtype: float64\n",
      "Product\n",
      "Debt collection                0.036844\n",
      "Credit card or prepaid card    0.036560\n",
      "Name: topic_4, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# check the average weight of each topic across Product by using groupby\n",
    "grouped = cfpb_topics.groupby('Product')\n",
    "for i in range(0, N_TOPICS):\n",
    "    print(grouped[f\"topic_{i}\"].mean().sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topics 1 and 4 seem to have a lot of separation between checkings or savings account and student loans. Let's plot them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 530.5x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.displot(cfpb_topics, x=cfpb_topics['topic_0'], hue = \"Product\", kind = 'kde', fill = 'true')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 530.5x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.displot(cfpb_topics, x=cfpb_topics['topic_3'], hue = \"Product\", kind = 'kde', fill = 'true')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a very clean separation! These two topics would be good candidates to pass to a supervised learning algorithm if we were interested in predicting the Product."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge\n",
    "\n",
    "Try retraining the LDA witha  different number of topics, say 10. What do you notice? How is this similar to issues we've seen with other clustering algorithms?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dense_matrix' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-92-305621076e21>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mlda\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLatentDirichletAllocation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mlda\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdense_matrix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mprint_top_words\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_top_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mtopic_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtopic\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcomponents_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dense_matrix' is not defined"
     ]
    }
   ],
   "source": [
    "lda = LatentDirichletAllocation(n_components=10, max_iter=20, random_state=0)\n",
    "lda = lda.fit(tfidf_matrix)\n",
    "\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"\\nTopic #{}:\".format(topic_idx))\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()\n",
    "    \n",
    "tf_feature_names = tf.get_feature_names()\n",
    "print_top_words(lda, tf_feature_names, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Polarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common area of research in the social sciences is thinking about the \"sentiment\" of a text. The [`TextBlob`](https://textblob.readthedocs.io/en/dev/quickstart.html) library gives us access to a pre-trained sentiment analysis model. Text might be characterized as \"positive,\" \"negative,\" or \"neutral\" on a [-1,1] scale with -1 being highly negative and 1 being highly positive. Before we look at the code, do you expect that the sentiment scores for these data should be negative or positive? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-134-65903247fbae>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cfpb_sample['tokens'] = cfpb_sample['Consumer complaint narrative'].map(lambda x: rem_punc_stop(x))\n",
      "<ipython-input-134-65903247fbae>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cfpb_sample['tokens'] = cfpb_sample['tokens'].map(lambda text: ' '.join(text))\n"
     ]
    }
   ],
   "source": [
    "# take cfpb, take just the first 5 rows and call cfpb_sample, create a column ['tokens'] that uses rem_punc_stop to tokenize, join the list of tokens\n",
    "cfpb_sample = cfpb[0:10000]\n",
    "cfpb_sample['tokens'] = cfpb_sample['Consumer complaint narrative'].map(lambda x: rem_punc_stop(x))\n",
    "cfpb_sample['tokens'] = cfpb_sample['tokens'].map(lambda text: ' '.join(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use TextBlob to create a column 'polarity' in cfpb_sample\n",
    "cfpb_sample['polarity'] = cfpb_sample['tokens'].map(lambda text: TextBlob(text).sentiment.polarity)\n",
    "cfpb_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot polarity in cfpb_sample using seaborn\n",
    "sns.displot(cfpb_sample, x=\"polarity\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's going on here? Why does sentiment look so close to neutral, or even slightly positive? We know that all of the narratives in this dataset are consumer **complaints**, so we should expect them to look somewhat negative. Let's look at the 5 most positive reviews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the 5 most positive complaints in cfpb_sample\n",
    "for complaint in cfpb_sample.nlargest(5, 'polarity')['Consumer complaint narrative']:\n",
    "    print(complaint + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do we have any words that are skewing things? Let's look at the sentiment score for this first comment, and the individual sentiments of the words in it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_complaint = \"the company said they are offering a covid relief program which I requested assistance and they are saying a balloon payment is owed in XXXX I called the company and I was told that if I can't make this payment they will be talking taking litigation steps how are people who have lost their job able to keep their homes\"\n",
    "print(\"overall polarity score is \", TextBlob(sample_complaint).sentiment.polarity)\n",
    "for word in sample_complaint.split():\n",
    "    print(word, TextBlob(word).sentiment.polarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only one word actually has a sentiment score (\"able\")! TextBlob's sentiment polarity is not a simple average of all of the sentiments in a string - this is why preprocessing is important and why you should validate these types of off-the-shelf methods. Let's take a look at the most negative reviews and see if these make sense:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the 5 most negative complaints in cfpb_sample\n",
    "for complaint in cfpb_sample.nsmallest(5, 'polarity')['Consumer complaint narrative']:\n",
    "    print(complaint + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_complaint = \"Navient is the worst company to ever exist. Website does not work. Do the people at customer service even work for navient??? They don't know anything about whats going on. Applied for a repayement plan and their website always says an error has occured.\"\n",
    "print(\"overall polarity score is \", TextBlob(sample_complaint).sentiment.polarity)\n",
    "for word in sample_complaint.split():\n",
    "    print(word, TextBlob(word).sentiment.polarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case we have a perfectly negative sentiment (-1.0), but again only one word is contributing - \"worst\". Sentiment polarity is a powerful tool, but not automatically suited to inference. That being said, maybe it can be helpful for distinguishing between labels. We can take a look at how polarity differs across mortgage and student loans:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot polarity in cfpb_sample by Product\n",
    "sns.displot(cfpb_sample, x=\"polarity\", hue = \"Product\", col = \"Product\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge\n",
    "\n",
    "In addition to sentiment polarity, TextBlob also has a method for determining how \"objective\" or \"subjective\" a piece of text is. Plot the objectivity measure by Product. Do these results make sense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate subjectivity using TextBlob for each complaint in cfpb_sample and save as a column 'subjectivity'\n",
    "cfpb_sample['subjectivity'] = cfpb_sample['tokens'].map(lambda text: TextBlob(text).sentiment.subjectivity)\n",
    "# plot subjectivity in cfpb_sample\n",
    "sns.displot(cfpb_sample, x=\"subjectivity\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot subjectivity in cfpb_sample by Product\n",
    "sns.displot(cfpb_sample, x=\"subjectivity\", hue = \"Product\", col = \"Product\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Supervised Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Machine Learning Workflow in sklearn\n",
    "\n",
    "We are almost ready to start making some predictions! We already saw unsupervised learning, and now we will do some supervised learning to predict the \"Product category.\" Supervised learning can be divided into `regression` and `classification`. Regression is used to predict continuous outcomes, while classification is used to predict discrete ones. Let's load our tfidf vectorized dataframe (the commented out code created the data, but takes a long time to run!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take cfpb, turn the 'Product' column into a target variable and binarize, then use the tf_idf matrix as the feature matrix, and split the data into training and testing sets\n",
    "'''\n",
    "X = tfidf_vector.fit_transform(cfpb['Consumer complaint narrative'])\n",
    "\n",
    "# save X to a pickle in data folder\n",
    "# import pickle\n",
    "# with open('data/X_train.pkl', 'wb') as f:\n",
    "#     pickle.dump(X_train, f)\n",
    "'''\n",
    "import pickle\n",
    "with open('data/X.pkl', 'rb') as f:\n",
    "    X = pickle.load(f)\n",
    "\n",
    "'''\n",
    "from urllib.request import urlopen\n",
    "import cloudpickle as cp\n",
    "\n",
    "X = cp.load(urlopen(\"https://github.com/Akesari12/mpi_summer_school/raw/main/data/X.pkl\")) \n",
    "'''\n",
    "\n",
    "y = cfpb['Product'].map(lambda x: 1 if x == 'Debt collection' else 0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "y = cfpb['Product'].map(lambda x: 1 if x == 'Debt collection' else 0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then train a machine learning model. We'll start with something you may have seen before, the logistic regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a logistic regression to predict y_train with default hyperparameters\n",
    "## import LogisticRegression from sklearn.linear_model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "## initialize LogisticRegression and save as lr\n",
    "lr = LogisticRegression()\n",
    "## fit lr to X_train and y_train\n",
    "lr.fit(X_train, y_train)\n",
    "## predict y_test from X_test and save as y_pred\n",
    "y_pred = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create y_test and then create a confusion matrix for y_pred and y_test\n",
    "confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# create a heatmap of the confusion matrix and name the axes 'Debt collection', 'Credit card or prepaid card' and 'Predicted', 'Actual'. Don't use scientific notation\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "ax = plt.subplot()\n",
    "sns.heatmap(cm, annot=True, ax=ax, fmt='g')\n",
    "ax.set_xlabel('Predicted')\n",
    "ax.set_ylabel('Actual')\n",
    "ax.set_title('Confusion Matrix')\n",
    "ax.xaxis.set_ticklabels(['Credit card or prepaid card', 'Debt collection'])\n",
    "ax.yaxis.set_ticklabels(['Credit card or prepaid card', 'Debt collection'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge: Hyperparameter Tuning\n",
    "\n",
    "Use our code from yesterday to search for the optimal machine learning model using hyperparameter tuning. Optimize a [logistic regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) by varying the `penalty` and `C` parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a hyperparameter grid for logistic regression checking L1, L2, and Elastic Net regularization and 10 values of C\n",
    "param_grid = {'penalty': ['l1', 'l2', 'elasticnet'], 'C': np.logspace(-4, 4, 10)}\n",
    "\n",
    "# import GridSearchCV from sklearn.model_selection\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# initialize GridSearchCV with the logistic regression, the hyperparameter grid, and 5-fold cross validation\n",
    "grid = GridSearchCV(lr, param_grid, cv=5)\n",
    "\n",
    "# fit grid to X_train and y_train\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# print the best hyperparameters\n",
    "print(grid.best_params_)\n",
    "\n",
    "# show the best 5 scores\n",
    "grid.cv_results_['mean_test_score'][0:5]\n",
    "\n",
    "# print the best score\n",
    "print(grid.best_score_)\n",
    "\n",
    "# save the best estimator as best_lr\n",
    "best_lr = grid.best_estimator_\n",
    "\n",
    "# confusion matrix for the best estimator, label the axes 'Prediction' and 'Actual' and 'Credit card or prepaid card' and 'Debt collection'\n",
    "y_pred = best_lr.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "ax = plt.subplot()\n",
    "sns.heatmap(cm, annot=True, ax=ax, fmt='g')\n",
    "ax.set_xlabel('Predicted')\n",
    "ax.set_ylabel('Actual')\n",
    "ax.set_title('Confusion Matrix')\n",
    "ax.xaxis.set_ticklabels(['Credit card or prepaid card', 'Debt collection'])\n",
    "ax.yaxis.set_ticklabels(['Credit card or prepaid card', 'Debt collection'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How did our optimal model do compared to the one without any hyperparameters set? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional Challenge\n",
    "\n",
    "Use our code from yesterday to search for the optimal machine learning model using hyperparameter tuning. Optimize a [logistic regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) by varying the `penalty` and `C` parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# create hyperparameter grids for logistic regression, random forest, and gradient boosting\n",
    "lr_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "rf_grid = {'n_estimators': [100, 200, 300], 'max_depth': [10, 20, 30]}\n",
    "gb_grid = {'n_estimators': [100, 200, 300], 'max_depth': [10, 20, 30], 'learning_rate': [0.1, 0.5, 1]}\n",
    "\n",
    "# import GridSearchCV from sklearn.model_selection\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# initialize GridSearchCV for logistic regression, random forest, and gradient boosting\n",
    "lr_gs = GridSearchCV(LogisticRegression(), lr_grid)\n",
    "rf_gs = GridSearchCV(RandomForestClassifier(), rf_grid)\n",
    "gb_gs = GridSearchCV(GradientBoostingClassifier(), gb_grid)\n",
    "\n",
    "# fit GridSearchCV for logistic regression, random forest, and gradient boosting\n",
    "lr_gs.fit(X_train, y_train)\n",
    "rf_gs.fit(X_train, y_train)\n",
    "gb_gs.fit(X_train, y_train)\n",
    "\n",
    "# print the best hyperparameters for logistic regression, random forest, and gradient boosting\n",
    "print(lr_gs.best_params_)\n",
    "print(rf_gs.best_params_)\n",
    "print(gb_gs.best_params_)\n",
    "\n",
    "# predict y_test from X_test using the best hyperparameters for logistic regression, random forest, and gradient boosting\n",
    "y_pred_lr = lr_gs.predict(X_test)\n",
    "y_pred_rf = rf_gs.predict(X_test)\n",
    "y_pred_gb = gb_gs.predict(X_test)\n",
    "\n",
    "# create confusion matrices for logistic regression, random forest, and gradient boosting\n",
    "cm_lr = confusion_matrix(y_test, y_pred_lr)\n",
    "cm_rf = confusion_matrix(y_test, y_pred_rf)\n",
    "cm_gb = confusion_matrix(y_test, y_pred_gb)\n",
    "\n",
    "# create heatmaps for logistic regression, random forest, and gradient boosting\n",
    "sn.heatmap(cm_lr, annot=True, cmap='Blues', fmt='g')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()\n",
    "\n",
    "sn.heatmap(cm_rf, annot=True, cmap='Blues', fmt='g')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()\n",
    "\n",
    "sn.heatmap(cm_gb, annot=True, cmap='Blues', fmt='g')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "\n",
    "plt.show()\n",
    "# print the accuracy scores for logistic regression, random forest, and gradient boosting\n",
    "print(lr_gs.score(X_test, y_test))\n",
    "print(rf_gs.score(X_test, y_test))\n",
    "print(gb_gs.score(X_test, y_test))\n",
    "# print the classification reports for logistic regression, random forest, and gradient boosting\n",
    "print(classification_report(y_test, y_pred_lr))\n",
    "print(classification_report(y_test, y_pred_rf))\n",
    "print(classification_report(y_test, y_pred_gb))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview <a id='context'></a>\n",
    "\n",
    "In this part, we will be turning individual words in the data set into vectors, called \"Word Embeddings\". Word embedding attempt to identify semantic relationships between words by observing them in the context that the word appears. Word2Vec is the most prominent word embedding algorithm.\n",
    "\n",
    "Imagine that each word in a novel has its meaning determined by the ones that surround it in a limited window. For example, in Moby Dick's first sentence, “me” is paired on either side by “Call” and “Ishmael.” After observing the windows around every word in the novel (or many novels), the computer will notice a pattern in which “me” falls between similar pairs of words to “her,” “him,” or “them.” Of course, the computer had gone through a similar process over the words “Call” and “Ishmael,” for which “me” is reciprocally part of their contexts.  This chaining of signifiers to one another mirrors some of humanists' most sophisticated interpretative frameworks of language.\n",
    "\n",
    "The two main flavors of Word2Vec are CBOW (Continuous Bag of Words) and Skip-Gram, which can be distinguished partly by their input and output during training. Skip-Gram takes a word of interest as its input (e.g. \"me\") and tries to learn how to predict its context words (\"Call\",\"Ishmael\"). CBOW does the opposite, taking the context words (\"Call\",\"Ishmael\") as a single input and tries to predict the word of interest (\"me\").\n",
    "\n",
    "In general, CBOW is is faster and does well with frequent words, while Skip-Gram potentially represents rare words better.\n",
    "\n",
    "### Word2Vec Features\n",
    "\n",
    "* `size`: Number of dimensions for word embedding model\n",
    "* `window`: Number of context words to observe in each direction\n",
    "* `min_count`: Minimum frequency for words included in model\n",
    "* `sg` (Skip-Gram): '0' indicates CBOW model; '1' indicates Skip-Gram\n",
    "* `alpha`: Learning rate (initial); prevents model from over-correcting, enables finer tuning\n",
    "* `iterations`: Number of passes through dataset\n",
    "* `batch_words`: Number of words to sample from data during each pass\n",
    "\n",
    "\n",
    "For more detailed background on Word2Vec's mechanics, see this  <a href=\"https://www.tensorflow.org/versions/r0.8/tutorials/word2vec/index.html\">brief tutorial</a> by Google, especially the sections \"Motivation,\" \"Skip-Gram Model,\" and \"Visualizing.\"\n",
    "\n",
    "We will be using the default value for most of our parameters.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's use our handy preprocessing function. Notice that this version will return a list of tokens (not a string), and we also added the `str.lower()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rem_punc_stop(text):\n",
    "    stop_words = STOP_WORDS\n",
    "    # Individually\n",
    "    # nlp.Defaults.stop_words.add(\"XX\")\n",
    "    # nlp.Defaults.stop_words.add(\"XXXX\")\n",
    "    # nlp.Defaults.stop_words.add(\"XXXXXXX\")\n",
    "    \n",
    "    # Using the bitwise |= (or) operator\n",
    "    nlp.Defaults.stop_words |= {\"xx\", \"xxxx\",\"xxxxxxxx\"}\n",
    "    \n",
    "    punc = set(punctuation)\n",
    "    \n",
    "    punc_free = \"\".join([ch for ch in text if ch not in punc])\n",
    "    \n",
    "    doc = nlp(punc_free)\n",
    "    \n",
    "    spacy_words = [token.text.lower() for token in doc]\n",
    "    \n",
    "    no_punc = [word for word in spacy_words if word not in stop_words]\n",
    "    \n",
    "    return no_punc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-148-89e08ad5e2b9>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cfpb_sample['tokens'] = cfpb_sample[0:100]['Consumer complaint narrative'].map(lambda x: rem_punc_stop(x))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0       [money, held, permission, causing, pain, suffe...\n",
       "1       [defrauded, auto, mechanic, immediately, reach...\n",
       "2       [seen,  , department, received, 100,  , patien...\n",
       "3       [milestone, genesis, fs, card, services, offer...\n",
       "4       [debt, credit, report, reporting, opened, repo...\n",
       "                              ...                        \n",
       "9995                                                  NaN\n",
       "9996                                                  NaN\n",
       "9997                                                  NaN\n",
       "9998                                                  NaN\n",
       "9999                                                  NaN\n",
       "Name: tokens, Length: 10000, dtype: object"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfpb_sample['tokens'] = cfpb_sample['Consumer complaint narrative'].map(lambda x: rem_punc_stop(x))\n",
    "cfpb_sample['tokens']\n",
    "#cfpb_sample.to_csv('data/CFPB 2023 Sample.csv')\n",
    "\n",
    "#cfpb_sample = pd.read_csv('data/CFPB 2023 Sample.csv')\n",
    "\n",
    "#y = cfpb_sample['Product'].map(lambda x: 1 if x == 'Debt collection' else 0)\n",
    "#X = tfidf_vector.fit_transform(cfpb_sample['Consumer complaint narrative'])\n",
    "\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Date received</th>\n",
       "      <th>Product</th>\n",
       "      <th>Sub-product</th>\n",
       "      <th>Issue</th>\n",
       "      <th>Sub-issue</th>\n",
       "      <th>Consumer complaint narrative</th>\n",
       "      <th>Company public response</th>\n",
       "      <th>Company</th>\n",
       "      <th>State</th>\n",
       "      <th>...</th>\n",
       "      <th>Tags</th>\n",
       "      <th>Consumer consent provided?</th>\n",
       "      <th>Submitted via</th>\n",
       "      <th>Date sent to company</th>\n",
       "      <th>Company response to consumer</th>\n",
       "      <th>Timely response?</th>\n",
       "      <th>Consumer disputed?</th>\n",
       "      <th>Complaint ID</th>\n",
       "      <th>year_received</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2023-07-22</td>\n",
       "      <td>Credit card or prepaid card</td>\n",
       "      <td>General-purpose credit card or charge card</td>\n",
       "      <td>Problem with a purchase shown on your statement</td>\n",
       "      <td>Credit card company isn't resolving a dispute ...</td>\n",
       "      <td>Money is being held with out my permission. Ca...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Chime Financial Inc</td>\n",
       "      <td>CA</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Consent provided</td>\n",
       "      <td>Web</td>\n",
       "      <td>2023-07-22</td>\n",
       "      <td>Closed with explanation</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7291298</td>\n",
       "      <td>2023</td>\n",
       "      <td>[money, held, permission, causing, pain, suffe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>2023-08-12</td>\n",
       "      <td>Credit card or prepaid card</td>\n",
       "      <td>General-purpose credit card or charge card</td>\n",
       "      <td>Problem with a purchase shown on your statement</td>\n",
       "      <td>Credit card company isn't resolving a dispute ...</td>\n",
       "      <td>I was defrauded by an auto mechanic in XXXX an...</td>\n",
       "      <td>Company has responded to the consumer and the ...</td>\n",
       "      <td>BANK OF AMERICA, NATIONAL ASSOCIATION</td>\n",
       "      <td>FL</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Consent provided</td>\n",
       "      <td>Web</td>\n",
       "      <td>2023-08-12</td>\n",
       "      <td>Closed with explanation</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7389759</td>\n",
       "      <td>2023</td>\n",
       "      <td>[defrauded, auto, mechanic, immediately, reach...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15</td>\n",
       "      <td>2023-07-20</td>\n",
       "      <td>Debt collection</td>\n",
       "      <td>Medical debt</td>\n",
       "      <td>Attempts to collect debt not owed</td>\n",
       "      <td>Debt was paid</td>\n",
       "      <td>On XX/XX/XXXX I was seen at the XXXX XXXX  dep...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Law Offices of Mitchell D. Bluhm &amp; Associates</td>\n",
       "      <td>MI</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Consent provided</td>\n",
       "      <td>Web</td>\n",
       "      <td>2023-07-20</td>\n",
       "      <td>Closed with non-monetary relief</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7279944</td>\n",
       "      <td>2023</td>\n",
       "      <td>[seen,  , department, received, 100,  , patien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17</td>\n",
       "      <td>2023-07-17</td>\n",
       "      <td>Debt collection</td>\n",
       "      <td>Credit card debt</td>\n",
       "      <td>Attempts to collect debt not owed</td>\n",
       "      <td>Debt was paid</td>\n",
       "      <td>Milestone through Genesis FS card services off...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Genesis FS Card Services, Inc.</td>\n",
       "      <td>KS</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Consent provided</td>\n",
       "      <td>Web</td>\n",
       "      <td>2023-07-17</td>\n",
       "      <td>Closed with explanation</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7261717</td>\n",
       "      <td>2023</td>\n",
       "      <td>[milestone, genesis, fs, card, services, offer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18</td>\n",
       "      <td>2023-07-21</td>\n",
       "      <td>Debt collection</td>\n",
       "      <td>Payday loan debt</td>\n",
       "      <td>Attempts to collect debt not owed</td>\n",
       "      <td>Debt was paid</td>\n",
       "      <td>I have a debt on my credit report reporting as...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ENOVA INTERNATIONAL, INC.</td>\n",
       "      <td>GA</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Consent provided</td>\n",
       "      <td>Web</td>\n",
       "      <td>2023-07-21</td>\n",
       "      <td>Closed with explanation</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7286377</td>\n",
       "      <td>2023</td>\n",
       "      <td>[debt, credit, report, reporting, opened, repo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 Date received                      Product  \\\n",
       "0           0    2023-07-22  Credit card or prepaid card   \n",
       "1           8    2023-08-12  Credit card or prepaid card   \n",
       "2          15    2023-07-20              Debt collection   \n",
       "3          17    2023-07-17              Debt collection   \n",
       "4          18    2023-07-21              Debt collection   \n",
       "\n",
       "                                  Sub-product  \\\n",
       "0  General-purpose credit card or charge card   \n",
       "1  General-purpose credit card or charge card   \n",
       "2                                Medical debt   \n",
       "3                            Credit card debt   \n",
       "4                            Payday loan debt   \n",
       "\n",
       "                                             Issue  \\\n",
       "0  Problem with a purchase shown on your statement   \n",
       "1  Problem with a purchase shown on your statement   \n",
       "2                Attempts to collect debt not owed   \n",
       "3                Attempts to collect debt not owed   \n",
       "4                Attempts to collect debt not owed   \n",
       "\n",
       "                                           Sub-issue  \\\n",
       "0  Credit card company isn't resolving a dispute ...   \n",
       "1  Credit card company isn't resolving a dispute ...   \n",
       "2                                      Debt was paid   \n",
       "3                                      Debt was paid   \n",
       "4                                      Debt was paid   \n",
       "\n",
       "                        Consumer complaint narrative  \\\n",
       "0  Money is being held with out my permission. Ca...   \n",
       "1  I was defrauded by an auto mechanic in XXXX an...   \n",
       "2  On XX/XX/XXXX I was seen at the XXXX XXXX  dep...   \n",
       "3  Milestone through Genesis FS card services off...   \n",
       "4  I have a debt on my credit report reporting as...   \n",
       "\n",
       "                             Company public response  \\\n",
       "0                                                NaN   \n",
       "1  Company has responded to the consumer and the ...   \n",
       "2                                                NaN   \n",
       "3                                                NaN   \n",
       "4                                                NaN   \n",
       "\n",
       "                                             Company State  ... Tags  \\\n",
       "0                                Chime Financial Inc    CA  ...  NaN   \n",
       "1              BANK OF AMERICA, NATIONAL ASSOCIATION    FL  ...  NaN   \n",
       "2  The Law Offices of Mitchell D. Bluhm & Associates    MI  ...  NaN   \n",
       "3                     Genesis FS Card Services, Inc.    KS  ...  NaN   \n",
       "4                          ENOVA INTERNATIONAL, INC.    GA  ...  NaN   \n",
       "\n",
       "  Consumer consent provided? Submitted via Date sent to company  \\\n",
       "0           Consent provided           Web           2023-07-22   \n",
       "1           Consent provided           Web           2023-08-12   \n",
       "2           Consent provided           Web           2023-07-20   \n",
       "3           Consent provided           Web           2023-07-17   \n",
       "4           Consent provided           Web           2023-07-21   \n",
       "\n",
       "      Company response to consumer Timely response? Consumer disputed?  \\\n",
       "0          Closed with explanation              Yes                NaN   \n",
       "1          Closed with explanation              Yes                NaN   \n",
       "2  Closed with non-monetary relief              Yes                NaN   \n",
       "3          Closed with explanation              Yes                NaN   \n",
       "4          Closed with explanation              Yes                NaN   \n",
       "\n",
       "   Complaint ID  year_received  \\\n",
       "0       7291298           2023   \n",
       "1       7389759           2023   \n",
       "2       7279944           2023   \n",
       "3       7261717           2023   \n",
       "4       7286377           2023   \n",
       "\n",
       "                                              tokens  \n",
       "0  [money, held, permission, causing, pain, suffe...  \n",
       "1  [defrauded, auto, mechanic, immediately, reach...  \n",
       "2  [seen,  , department, received, 100,  , patien...  \n",
       "3  [milestone, genesis, fs, card, services, offer...  \n",
       "4  [debt, credit, report, reporting, opened, repo...  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfpb_sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have pre-processed our text, we can use the [`gensim`](https://radimrehurek.com/gensim/) library to construct our word embeddings. We will use the Continous Bag of Words model (CBOW), which predicts target words from its neighboring context words to learn word embeddings from raw text.\n",
    "\n",
    "Read through the documentation of the Word2Vec method in gensim to understand how to implement the Word2Vec model. Then fill in the blanks so that: we use a __Continuous Bag of Words__ model to create word embeddings of __size 100__ for words that appear in `text` __5 or more times__. We'll set the learning rate to .025, and sample 10000 words from the data during each pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'float' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-150-e3f29432e270>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m model = gensim.models.Word2Vec(cfpb_sample['tokens'], vector_size=100, window=5, \n\u001b[0m\u001b[0;32m      2\u001b[0m                                min_count=5, sg=0, alpha=0.025, batch_words=10000)\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\word2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, sentences, corpus_file, vector_size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, ns_exponent, cbow_mean, hashfxn, epochs, null_word, trim_rule, sorted_vocab, batch_words, compute_loss, callbacks, comment, max_final_vocab, shrink_windows)\u001b[0m\n\u001b[0;32m    423\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcorpus_iterable\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mcorpus_file\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    424\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_corpus_sanity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpasses\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 425\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    426\u001b[0m             self.train(\n\u001b[0;32m    427\u001b[0m                 \u001b[0mcorpus_iterable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus_count\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\word2vec.py\u001b[0m in \u001b[0;36mbuild_vocab\u001b[1;34m(self, corpus_iterable, corpus_file, update, progress_per, keep_raw_vocab, trim_rule, **kwargs)\u001b[0m\n\u001b[0;32m    485\u001b[0m         \"\"\"\n\u001b[0;32m    486\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_corpus_sanity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpasses\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 487\u001b[1;33m         total_words, corpus_count = self.scan_vocab(\n\u001b[0m\u001b[0;32m    488\u001b[0m             corpus_iterable=corpus_iterable, corpus_file=corpus_file, progress_per=progress_per, trim_rule=trim_rule)\n\u001b[0;32m    489\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcorpus_count\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\word2vec.py\u001b[0m in \u001b[0;36mscan_vocab\u001b[1;34m(self, corpus_iterable, corpus_file, progress_per, workers, trim_rule)\u001b[0m\n\u001b[0;32m    580\u001b[0m             \u001b[0mcorpus_iterable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLineSentence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    581\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 582\u001b[1;33m         \u001b[0mtotal_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_scan_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprogress_per\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    583\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    584\u001b[0m         logger.info(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\word2vec.py\u001b[0m in \u001b[0;36m_scan_vocab\u001b[1;34m(self, sentences, progress_per, trim_rule)\u001b[0m\n\u001b[0;32m    563\u001b[0m                     \u001b[0msentence_no\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    564\u001b[0m                 )\n\u001b[1;32m--> 565\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    566\u001b[0m                 \u001b[0mvocab\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    567\u001b[0m             \u001b[0mtotal_words\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'float' object is not iterable"
     ]
    }
   ],
   "source": [
    "model = gensim.models.Word2Vec(cfpb_sample['tokens'], vector_size=100, window=5, \n",
    "                               min_count=5, sg=0, alpha=0.025, batch_words=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings <a id='subsection 2'></a>\n",
    "\n",
    "Now that we've trained the mode, we can return the actual high-dimensional vector by simply indexing the model with the word as the key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Key 'account' not present\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-151-8d5ca58ecc64>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# New syntax as of gensim 4.0.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# model['account'] works for 3.8.3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'account'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key_or_keys)\u001b[0m\n\u001b[0;32m    395\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey_or_keys\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    396\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 397\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mvstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mkey_or_keys\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    398\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    399\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    395\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey_or_keys\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    396\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 397\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mvstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mkey_or_keys\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    398\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    399\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mget_vector\u001b[1;34m(self, key, norm)\u001b[0m\n\u001b[0;32m    436\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    437\u001b[0m         \"\"\"\n\u001b[1;32m--> 438\u001b[1;33m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    439\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnorm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    440\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfill_norms\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mget_index\u001b[1;34m(self, key, default)\u001b[0m\n\u001b[0;32m    410\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mdefault\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    411\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 412\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Key '{key}' not present\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    413\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    414\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"Key 'account' not present\""
     ]
    }
   ],
   "source": [
    "# New syntax as of gensim 4.0.0\n",
    "# model['account'] works for 3.8.3\n",
    "print(model.wv.__getitem__(['account']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out the shape of the vectors for 'account', what do you notice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Key 'account' not present\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-137-1890988b34e2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'account'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key_or_keys)\u001b[0m\n\u001b[0;32m    395\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey_or_keys\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    396\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 397\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mvstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mkey_or_keys\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    398\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    399\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    395\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey_or_keys\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    396\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 397\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mvstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mkey_or_keys\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    398\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    399\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mget_vector\u001b[1;34m(self, key, norm)\u001b[0m\n\u001b[0;32m    436\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    437\u001b[0m         \"\"\"\n\u001b[1;32m--> 438\u001b[1;33m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    439\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnorm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    440\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfill_norms\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mget_index\u001b[1;34m(self, key, default)\u001b[0m\n\u001b[0;32m    410\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mdefault\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    411\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 412\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Key '{key}' not present\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    413\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    414\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"Key 'account' not present\""
     ]
    }
   ],
   "source": [
    "model.wv.__getitem__(['account']).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: There are 100 elements in this array. Notice that this was one of the parameters we set in the training process!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the following empty cells to look at what the word embeddings look like for words you think may appear in the text! Keep in mind that even if a word shows up in the text as seen above, a word vector will not be created unless it satisfies all conditions we inputted into the model above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Key 'mastercard' not present\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-138-5c6382322993>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# word 1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'mastercard'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key_or_keys)\u001b[0m\n\u001b[0;32m    395\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey_or_keys\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    396\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 397\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mvstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mkey_or_keys\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    398\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    399\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    395\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey_or_keys\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    396\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 397\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mvstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mkey_or_keys\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    398\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    399\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mget_vector\u001b[1;34m(self, key, norm)\u001b[0m\n\u001b[0;32m    436\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    437\u001b[0m         \"\"\"\n\u001b[1;32m--> 438\u001b[1;33m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    439\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnorm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    440\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfill_norms\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mget_index\u001b[1;34m(self, key, default)\u001b[0m\n\u001b[0;32m    410\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mdefault\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    411\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 412\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Key '{key}' not present\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    413\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    414\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"Key 'mastercard' not present\""
     ]
    }
   ],
   "source": [
    "# word 1\n",
    "model.wv.__getitem__(['mastercard'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Key 'company' not present\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-139-73e544307e8b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# word 2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'company'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key_or_keys)\u001b[0m\n\u001b[0;32m    395\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey_or_keys\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    396\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 397\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mvstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mkey_or_keys\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    398\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    399\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    395\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey_or_keys\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    396\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 397\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mvstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mkey_or_keys\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    398\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    399\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mget_vector\u001b[1;34m(self, key, norm)\u001b[0m\n\u001b[0;32m    436\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    437\u001b[0m         \"\"\"\n\u001b[1;32m--> 438\u001b[1;33m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    439\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnorm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    440\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfill_norms\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mget_index\u001b[1;34m(self, key, default)\u001b[0m\n\u001b[0;32m    410\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mdefault\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    411\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 412\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Key '{key}' not present\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    413\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    414\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"Key 'company' not present\""
     ]
    }
   ],
   "source": [
    "# word 2\n",
    "model.wv.__getitem__(['company'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`gensim` comes with some handy methods to analyze word relationships. `similarity` will give us a number from 0-1 based on how similar two words are. If this sounds like cosine similarity for words, you'd be right! It just takes the cosine similarity of the high dimensional vectors we input. \n",
    "\n",
    "In the following cell, find the similarity between the words `credit` and `debt`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Key 'credit' not present\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-140-b7d2af6c674d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# New syntax as of gensim 4.0.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# model.similarity() works as of 3.8.3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msimilarity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'credit'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'debt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36msimilarity\u001b[1;34m(self, w1, w2)\u001b[0m\n\u001b[0;32m   1152\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1153\u001b[0m         \"\"\"\n\u001b[1;32m-> 1154\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmatutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munitvec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mw1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmatutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munitvec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mw2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1155\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1156\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mn_similarity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mws1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mws2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key_or_keys)\u001b[0m\n\u001b[0;32m    393\u001b[0m         \"\"\"\n\u001b[0;32m    394\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey_or_keys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_KEY_TYPES\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 395\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey_or_keys\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    396\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    397\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mvstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mkey_or_keys\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mget_vector\u001b[1;34m(self, key, norm)\u001b[0m\n\u001b[0;32m    436\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    437\u001b[0m         \"\"\"\n\u001b[1;32m--> 438\u001b[1;33m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    439\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnorm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    440\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfill_norms\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mget_index\u001b[1;34m(self, key, default)\u001b[0m\n\u001b[0;32m    410\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mdefault\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    411\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 412\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Key '{key}' not present\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    413\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    414\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"Key 'credit' not present\""
     ]
    }
   ],
   "source": [
    "# New syntax as of gensim 4.0.0\n",
    "# model.similarity() works as of 3.8.3\n",
    "model.wv.similarity('credit', 'debt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also find cosine distance between two clusters of word vectors. Each cluster is measured as the mean of its words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Key 'credit' not present\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-141-0cef646a6df8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Similarity between credit/debt and loan/mortgage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_similarity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'credit'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'debt'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'loan'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'mortgage'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mn_similarity\u001b[1;34m(self, ws1, ws2)\u001b[0m\n\u001b[0;32m   1172\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mws1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mws2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1173\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mZeroDivisionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'At least one of the passed list is empty.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1174\u001b[1;33m         \u001b[0mv1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mws1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1175\u001b[0m         \u001b[0mv2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mws2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1176\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmatutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munitvec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmatutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munitvec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1172\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mws1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mws2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1173\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mZeroDivisionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'At least one of the passed list is empty.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1174\u001b[1;33m         \u001b[0mv1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mws1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1175\u001b[0m         \u001b[0mv2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mws2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1176\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmatutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munitvec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmatutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munitvec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key_or_keys)\u001b[0m\n\u001b[0;32m    393\u001b[0m         \"\"\"\n\u001b[0;32m    394\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey_or_keys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_KEY_TYPES\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 395\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey_or_keys\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    396\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    397\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mvstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mkey_or_keys\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mget_vector\u001b[1;34m(self, key, norm)\u001b[0m\n\u001b[0;32m    436\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    437\u001b[0m         \"\"\"\n\u001b[1;32m--> 438\u001b[1;33m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    439\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnorm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    440\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfill_norms\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mget_index\u001b[1;34m(self, key, default)\u001b[0m\n\u001b[0;32m    410\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mdefault\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    411\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 412\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Key '{key}' not present\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    413\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    414\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"Key 'credit' not present\""
     ]
    }
   ],
   "source": [
    "# Similarity between credit/debt and loan/mortgage\n",
    "model.wv.n_similarity(['credit','debt'],['loan','mortgage'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can find words that don't belong with `doesnt_match`. It finds the mean vector of the words in the `list`, and identifies the furthest away. Out of the three words in the list `['credit', 'loan', 'student']`, which is the furthest vector from the mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot select a word from an empty list",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-142-e2737faf6737>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# New syntax for 4.0.0 requires passing a tuple ()  a list []\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# ['credit', 'loan', 'student'] works for 3.8.3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdoesnt_match\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'credit'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'loan'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'student'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mdoesnt_match\u001b[1;34m(self, words)\u001b[0m\n\u001b[0;32m   1058\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1059\u001b[0m         \"\"\"\n\u001b[1;32m-> 1060\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrank_by_centrality\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1061\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1062\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mrank_by_centrality\u001b[1;34m(self, words, use_norm)\u001b[0m\n\u001b[0;32m   1038\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"vectors for words %s are not present in the model, ignoring these words\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignored_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1039\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mused_words\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1040\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"cannot select a word from an empty list\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1041\u001b[0m         \u001b[0mvectors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_norm\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mused_words\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mREAL\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1042\u001b[0m         \u001b[0mmean\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmatutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munitvec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvectors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mREAL\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: cannot select a word from an empty list"
     ]
    }
   ],
   "source": [
    "# New syntax for 4.0.0 requires passing a tuple ()  a list []\n",
    "# ['credit', 'loan', 'student'] works for 3.8.3\n",
    "model.wv.doesnt_match(['credit', 'loan', 'student'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most famous implementation of this vector math is semantics. What happens if we take:\n",
    "\n",
    "$$\\vec{house} - \\vec{rent} + \\vec{loan} = $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Key 'house' not present\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-143-d78cd706226f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpositive\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'house'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'loan'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'rent'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mmost_similar\u001b[1;34m(self, positive, negative, topn, clip_start, clip_end, restrict_vocab, indexer)\u001b[0m\n\u001b[0;32m    771\u001b[0m                 \u001b[0mmean\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    772\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 773\u001b[1;33m                 \u001b[0mmean\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    774\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhas_index_for\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    775\u001b[0m                     \u001b[0mall_keys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mget_vector\u001b[1;34m(self, key, norm)\u001b[0m\n\u001b[0;32m    436\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    437\u001b[0m         \"\"\"\n\u001b[1;32m--> 438\u001b[1;33m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    439\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnorm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    440\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfill_norms\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mget_index\u001b[1;34m(self, key, default)\u001b[0m\n\u001b[0;32m    410\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mdefault\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    411\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 412\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Key '{key}' not present\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    413\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    414\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"Key 'house' not present\""
     ]
    }
   ],
   "source": [
    "model.wv.most_similar(positive=['house', 'loan'], negative=['rent'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification with BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A major innovation that improves on word2vec is the advent of <b>transformers</b> for natural language processing. First introduced by the now-famous paper, [Attention is All You Need](https://arxiv.org/abs/1706.03762), this technology basically improves on other models by weighting how important different parts of a sentence are. While word2vec works primarily by looking for the co-occurrence of words near each other, BERT improves on this by learning weights for how words modify each other, contribute the overall meaning of a sentence etc. \n",
    "\n",
    "Below you will see code for loading and running a BERT model for classification. We won't actually run all of this because it takes a long time to run but you should try this on your own at home!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_train = label_encoder.fit_transform(y_train)\n",
    "\n",
    "# Check the unique labels and their corresponding encoded values\n",
    "label_encoder.classes_, label_encoder.transform(label_encoder.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load the BERT model and tokenizer**\n",
    "\n",
    "We will use the `transformers` library to load the Legal_BERT model and tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer and model from Hugging Face\n",
    "model_name = \"nlpaueb/legal-bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(label_encoder.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 1: Use Legal_BERT as is for prediction**\n",
    "\n",
    "In this part, we will use the pre-trained Legal_BERT model without any fine-tuning to predict the labels on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.texts.shape[0]\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        text = self.texts[item].toarray().flatten()  # Convert sparse row to dense array and flatten it\n",
    "        label = self.labels[item]\n",
    "\n",
    "        # Convert array to string\n",
    "        text = ' '.join(map(str, text))\n",
    "\n",
    "        # Ensure text is a string\n",
    "        if not isinstance(text, str):\n",
    "            raise ValueError(f\"Expected string but got {type(text)}\")\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',  # Use 'max_length' instead of pad_to_max_length\n",
    "            truncation=True,       # Explicitly set truncation\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'text': text,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "\n",
    "# Parameters\n",
    "MAX_LEN = 128\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# train_dataset = TextDataset(train_texts, train_labels, tokenizer, MAX_LEN)\n",
    "val_dataset = TextDataset(val_texts, val_labels, tokenizer, MAX_LEN)\n",
    "\n",
    "# train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=32)\n",
    "\n",
    "# Initialize dataset and dataloader\n",
    "train_dataset = TextDataset(X_train, y_train, tokenizer, MAX_LEN)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "val_dataset = TextDataset(val_texts, val_labels, tokenizer, MAX_LEN)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Evaluation function\n",
    "def eval_model(model, data_loader, device):\n",
    "    model = model.eval()\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for d in data_loader:\n",
    "            input_ids = d[\"input_ids\"].to(device)\n",
    "            attention_mask = d[\"attention_mask\"].to(device)\n",
    "            labels = d[\"label\"].to(device)\n",
    "            \n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "            \n",
    "            _, preds = torch.max(logits, dim=1)\n",
    "            correct_predictions += torch.sum(preds == labels)\n",
    "            losses.append(loss.item())\n",
    "    \n",
    "    return correct_predictions.double() / len(data_loader.dataset), np.mean(losses)\n",
    "\n",
    "# Evaluate the pre-trained model on the validation set\n",
    "val_acc, val_loss = eval_model(model, train_dataloader, device)\n",
    "print(f'Validation loss (pre-trained model): {val_loss}')\n",
    "print(f'Validation accuracy (pre-trained model): {val_acc}')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 2: Fine-tune Legal_BERT and compare the results**\n",
    "\n",
    "In this part, we will fine-tune the Legal_BERT model on our dataset and then predict the labels on the validation set to compare the results with the pre-trained model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine-tuning involves training the pre-trained model on our specific dataset to adjust the model weights based on our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Optimizer and scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n",
    "total_steps = len(train_dataloader) * 3  # 3 epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "# Training function\n",
    "def train_epoch(model, data_loader, optimizer, device, scheduler):\n",
    "    model = model.train()\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "    \n",
    "    for d in data_loader:\n",
    "        input_ids = d[\"input_ids\"].to(device)\n",
    "        attention_mask = d[\"attention_mask\"].to(device)\n",
    "        labels = d[\"label\"].to(device)\n",
    "        \n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        _, preds = torch.max(logits, dim=1)\n",
    "        correct_predictions += torch.sum(preds == labels)\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "    return correct_predictions.double() / len(data_loader.dataset), np.mean(losses)\n",
    "\n",
    "# Training loop\n",
    "EPOCHS = 3\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
    "    print('-' * 10)\n",
    "    \n",
    "    train_acc, train_loss = train_epoch(model, train_dataloader, optimizer, device, scheduler)\n",
    "    print(f'Train loss {train_loss} accuracy {train_acc}')\n",
    "    \n",
    "    val_acc, val_loss = eval_model(model, val_dataloader, device)\n",
    "    print(f'Val loss {val_loss} accuracy {val_acc}')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT API Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, set up your API Key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from pathlib import Path\n",
    "\n",
    "# Open and read the text file, saving the content as a string\n",
    "with open(\"openai_key.txt\", 'r') as file:\n",
    "    file_text = file.read()  # Read the entire content into a string\n",
    "\n",
    "client = OpenAI(\n",
    "  api_key=file_text,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use the openai API to classify the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_choice = \"gpt-4o-mini\"\n",
    "\n",
    "def classify_narrative_zeroshot(narrative):\n",
    "    response = client.chat.completions.create(\n",
    "      model=model_choice,\n",
    "      messages=[\n",
    "        {\n",
    "          \"role\": \"system\",\n",
    "          \"content\": [\n",
    "            {\n",
    "              \"type\": \"text\",\n",
    "              \"text\": \"Which of the following product categories is the following text discussing? \\n 'Debt collection' or 'Credit card or prepaid card'. Respond only with the name of the category.\"\n",
    "            }\n",
    "          ]\n",
    "        },\n",
    "        {\n",
    "          \"role\": \"user\",\n",
    "          \"content\": [\n",
    "            {\n",
    "              \"type\": \"text\",\n",
    "              \"text\": f\"narrative: '{narrative}'\"\n",
    "            }\n",
    "          ]\n",
    "        }\n",
    "      ],\n",
    "      temperature=0,\n",
    "      max_tokens=256,\n",
    "      top_p=1,\n",
    "      frequency_penalty=0,\n",
    "      presence_penalty=0\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = cfpb['Consumer complaint narrative']\n",
    "labels = cfpb['Product']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = text[1:100]\n",
    "sample_labels = labels[1:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions\n",
    "classifications = sample_text.apply(classify_narrative_zeroshot)\n",
    "\n",
    "# Evaluate the model\n",
    "print(f\"{model_choice} Performance:\")\n",
    "print(classification_report(sample_labels, classifications))\n",
    "print(\"Accuracy:\", round(accuracy_score(sample_labels, classifications),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was the performance on zero-shot prompting. Now let's try one shot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_choice = \"gpt-4o-mini\"\n",
    "\n",
    "example_card = \"My credit card application was denied\"\n",
    "example_debt = \"This debt is from over XXXX ago\"\n",
    "\n",
    "def classify_narrative_oneshot(narrative):\n",
    "    response = client.chat.completions.create(\n",
    "      model=model_choice,\n",
    "      messages=[\n",
    "        {\n",
    "          \"role\": \"system\",\n",
    "          \"content\": [\n",
    "            {\n",
    "              \"type\": \"text\",\n",
    "              \"text\": f\"This complaint is about debt collection: {example_debt} and this is about credit cards: {example_card} \\nWhich of the following product categories is the following text discussing? \\n 'Debt collection' or 'Credit card or prepaid card'. Respond only with the name of the category.\"\n",
    "            }\n",
    "          ]\n",
    "        },\n",
    "        {\n",
    "          \"role\": \"user\",\n",
    "          \"content\": [\n",
    "            {\n",
    "              \"type\": \"text\",\n",
    "              \"text\": f\"narrative: '{narrative}'\"\n",
    "            }\n",
    "          ]\n",
    "        }\n",
    "      ],\n",
    "      temperature=0,\n",
    "      max_tokens=256,\n",
    "      top_p=1,\n",
    "      frequency_penalty=0,\n",
    "      presence_penalty=0\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions\n",
    "classifications = sample_text.apply(classify_narrative_oneshot)\n",
    "\n",
    "# Evaluate the model\n",
    "print(f\"{model_choice} Performance:\")\n",
    "print(classification_report(sample_labels, classifications))\n",
    "print(\"Accuracy one-shot:\", round(accuracy_score(sample_labels, classifications),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like we did better with zero-shot! Any ideas why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Future Directions\n",
    "\n",
    "We've covered some basics today, but here are some future steps you might take:\n",
    "\n",
    "* **Acquiring Data**: Working with web scrapers, APIs etc. to get structured text data.\n",
    "* **Preprocessing**: We did some basic pre-processing of text data, but there are other factors to consider like creating categorical dummies, scaling certain features, and doing some ex ante feature selection with methods like correlation plots.\n",
    "* **Regression**: We did a bit of classification, but some problems will require that you predict a continuous target and there are different techniques and metrics to go with this.\n",
    "* **Deep Learning**: We covered some basic classification algorithms, but neural nets are a powerful tool for making better classifications, especially with complex data like text or images.\n",
    "* **AutoML**: In practice, machine learning requires running hundreds or thousands of models to find the best one. AutoML libraries such as TPOT can help automate the process searching for the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge (time permitting)\n",
    "\n",
    "Do either of the following:\n",
    "\n",
    "a) Using your own dataset, implement some of the techniques we explored here to featurize text and predict some outcome.\n",
    "\n",
    "b) Using the CPFB dataset, try running some models that include non-tfidf features. For example, you might use topic weights, sentiment, or objectivity/subjectivity scores. You might also want to predict some of the other features in the dataset like whether the complaint was resolved, the issue, and the company involved. We didn't explore how to do automatic feature selection or other pre-processing - do you think you can get GPT to show you how to?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
