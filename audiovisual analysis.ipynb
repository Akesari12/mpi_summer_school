{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audiovisual Analysis of Courtroom Videos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the last part of our workshop, we will cover audiovisual analysis of courtroom data. The basic intuition underlying audio and video should feel familiar from your explorations of more standard machine learning and text analysis. In this part, we will see some of the standard parts of the audiovisual toolkit and preview some advances made possible by large language models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "\n",
    "# opencv\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "# librosa\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "# gdown\n",
    "import gdown\n",
    "\n",
    "# import moviepy\n",
    "import moviepy.editor as mp\n",
    "\n",
    "# import pydub\n",
    "from pydub import AudioSegment\n",
    "from pydub.playback import play\n",
    "\n",
    "# pyaudio and wave\n",
    "import pyaudio  \n",
    "import wave  \n",
    "\n",
    "# numpy\n",
    "import numpy as np\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt  \n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# misc\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import imutils\n",
    "from imutils import paths\n",
    "import shutil\n",
    "import base64\n",
    "import requests\n",
    "import re\n",
    "import json \n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "import pickle\n",
    "\n",
    "# deep learning\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.utils import to_categorical # added this import to encode labels as one-hot vectors\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "\n",
    "# whisper and soundfile\n",
    "import whisper\n",
    "import soundfile as sf\n",
    "\n",
    "# pyannote and pydub\n",
    "from pyannote.audio import Pipeline\n",
    "from pydub import AudioSegment\n",
    "from pyannote.core import Segment, Timeline, Annotation\n",
    "from pyannote.core import notebook\n",
    "\n",
    "# face emotion recognition\n",
    "from fer import Video\n",
    "from fer import FER\n",
    "\n",
    "# openai\n",
    "import openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audio Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start with audio. Our ultimate goal is to use audio to make predictions about who is speaking, and ultimately whether the way that they are speaking predicts how they will ultimately vote on a case. As with text data, the problem is that audio cannot be readily analyzed statistically - it needs to be transformed into <b>features</b> that can be used to train a machine learning model. Also like text, audio is high-dimensional which makes analysis computationally expensive. In this part, we will see some techniques for extracting audio features and analyzing them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate Audio from Video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first step is to separate an audio clip from its underlying video. Computer vision and computational audio analysis use different techniques and have a different \"unit\" of analysis. Whereas computer vision works on image at a time, with audio we are generally looking at longer segments of time. Let's start by extracting the audio from an example video:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a video clip and extract audio\n",
    "video_path = \"11-71541 Ara Hovanesyan v. Eric Holder, Jr.-QEvMv81TIQM.mp4\"\n",
    "audio_path = \"11-71541 Ara Hovanesyan v. Eric Holder, Jr.-QEvMv81TIQM.wav\"\n",
    "\n",
    "# load the video\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# clip audio\n",
    "clip = mp.VideoFileClip(video_path)\n",
    "# write audio to file\n",
    "clip.audio.write_audiofile(audio_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll go ahead and load the audio. `librosa.load()` returns two objects, a time series representing the audio and the \"sample rate\" or how often the audio clip was resampled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, sr = librosa.load(audio_path, res_type='kaiser_fast')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at the first 10 minutes. What do we see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the first 2000 samples\n",
    "first_ten_minutes = librosa.time_to_samples(600, sr=sr)\n",
    "intro = y[first_ten_minutes:]\n",
    "\n",
    "# Plot the waveform\n",
    "plt.figure(figsize=(10, 4))\n",
    "librosa.display.waveshow(y, sr=sr, max_points=50000, x_axis='time', offset=0.0)\n",
    "plt.title('Waveform')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do see that there are different amplitudes at various parts of the audio. Without more context, it is hard to say what is happening exactly though."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mel-Frequency Cepstral Coefficients "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You likely hear dozens or even hundreds of different voices each day. How do you distinguish between all of those different voices? While you're not aware of it, you distinguish between voices by using features such as pitch, tone, and timbre. Mel-Frequency Cepstral Coefficients (MFCC) are a set of features commonly used in audio processing, especially in speech recognition. At a high level, they take a sound and break it up into \"mels\" which are time units that mimic how humans perceive the pitch of a sound. MFCCs are computed by:\n",
    "\n",
    "1. Pre-Emphasis: A high-pass filter is applied to the audio signal to amplify high-frequency components. This balances the frequency spectrum and improves the signal-to-noise ratio.\n",
    "\n",
    "2. Framing: The continuous audio signal is divided into overlapping frames, typically 20-40 milliseconds long. This allows for analysis of short-term spectral properties of the audio signal.\n",
    "\n",
    "3. Windowing: Each frame is multiplied by a Hamming window to minimize discontinuities at the edges. This helps reduce spectral leakage when transforming the signal to the frequency domain.\n",
    "\n",
    "4. Fast Fourier Transform (FFT): The windowed frames are converted from the time domain to the frequency domain using FFT. This results in a representation of the signal in terms of its frequency components.\n",
    "\n",
    "5. Mel-Scale Filter Bank: The frequency domain signal is passed through a series of triangular filters spaced according to the Mel scale. The Mel scale is a perceptual scale of pitches judged by listeners to be equal in distance from one another, with a linear spacing below 1 kHz and a logarithmic spacing above 1 kHz. This step approximates the human ear's response to different frequencies.\n",
    "\n",
    "6. Logarithm of Power Spectrum: The power of each Mel-scaled frequency component is computed and transformed using a logarithm. This step simulates the human ear's logarithmic perception of loudness.\n",
    "\n",
    "7. Discrete Cosine Transform (DCT): The log-Mel power spectrum is then transformed using DCT to obtain the MFCCs. The DCT decorrelates the coefficients and packs the most significant features into the lower-order coefficients, which helps in reducing the dimensionality of the feature set.\n",
    "\n",
    "That's a lot of steps! The end result is that MFCCs provide a compact representation of the audio signal that retains essential information relevant to human perception, making them highly effective for tasks such as speech and speaker recognition. Lucky for us, we don't need to implement each of these steps. The `librosa` library does it all for us. The code below extracts the MFCCs then plots a mel-spectrogram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the Mel spectrogram\n",
    "spec = librosa.feature.melspectrogram(y=y, sr=sr)\n",
    "\n",
    "# Display the spectrogram\n",
    "plt.figure(figsize=(10, 4))\n",
    "librosa.display.specshow(librosa.power_to_db(spec, ref=np.max), y_axis='mel', x_axis='time', sr=sr)\n",
    "plt.colorbar(format='%+2.0f dB')\n",
    "plt.title('Mel spectrogram')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y-Axis (Frequency): The vertical axis represents frequency in the Mel scale. Lower frequencies are at the bottom, and higher frequencies are at the top. The Mel scale spaces frequencies in a way that reflects human auditory perception, with finer resolution at lower frequencies.\n",
    "\n",
    "X-Axis (Time): The horizontal axis represents time. As you move from left to right, you are looking at the progression of the audio signal over time.\n",
    "\n",
    "Color Intensity: The colors in the spectrogram represent the amplitude (power) of the signal at each frequency and time point.\n",
    "\n",
    "Black/Dark Colors: These indicate lower amplitude or quieter parts of the signal.\n",
    "Purple to Blue: Intermediate amplitude levels.\n",
    "Orange to Yellow: Higher amplitude or louder parts of the signal.\n",
    "The color bar on the right side of the spectrogram shows the scale of amplitude in decibels (dB), with darker colors indicating lower decibels and brighter colors indicating higher decibels.\n",
    "\n",
    "By interpreting the mel spectrogram, you can identify patterns in the frequency domain over time, such as the presence of certain tones, rhythms, and other acoustic features. This visualization is particularly useful for analyzing speech signals, musical notes, and other audio phenomena, providing insights into the structure and content of the audio data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Harmonic-Percussive Separation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sounds can also contain <b>harmonic</b> and <b>percussive</b> components. While these components are usually used in analyzing music, human speech also has harmonic and percussive components. We can separate the harmonic and percussive parts of an audio stream as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_h, data_p = librosa.effects.hpss(y)\n",
    "spec_h = librosa.feature.melspectrogram(y=data_h, sr=sr)\n",
    "spec_p = librosa.feature.melspectrogram(y=data_p, sr=sr)\n",
    "db_spec_h = librosa.power_to_db(spec_h,ref=np.max)\n",
    "db_spec_p = librosa.power_to_db(spec_p,ref=np.max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize the mel-frequencies of harmonic components like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "librosa.display.specshow(db_spec_h,y_axis='mel', x_axis='s', sr=sr)\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the percussive components like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "librosa.display.specshow(db_spec_p,y_axis='mel', x_axis='s', sr=sr)\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the amplitudes of both together like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y, sr = librosa.load(librosa.ex('choice'), duration=10)\n",
    "fig, ax = plt.subplots(nrows=3, sharex=True, sharey=True)\n",
    "librosa.display.waveshow(y, sr=sr, ax=ax[0])\n",
    "ax[0].set(title='Monophonic')\n",
    "ax[0].label_outer()\n",
    "\n",
    "#y, sr = librosa.load(librosa.ex('choice', hq=True), mono=False, duration=10)\n",
    "librosa.display.waveshow(y, sr=sr, ax=ax[1])\n",
    "ax[1].set(title='Stereo')\n",
    "ax[1].label_outer()\n",
    "\n",
    "#y, sr = librosa.load(librosa.ex('choice'), duration=10)\n",
    "y_harm, y_perc = librosa.effects.hpss(y)\n",
    "librosa.display.waveshow(y_harm, sr=sr, alpha=0.25, ax=ax[2])\n",
    "librosa.display.waveshow(y_perc, sr=sr, color='r', alpha=0.5, ax=ax[2])\n",
    "ax[2].set(title='Harmonic + Percussive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chroma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can further analyze pitch by analyzing the \"chroma.\" Chroma are related to the twelve classes within an octave (e.g. C#, D, etc.). We can extract the chroma and visualize it like this (don't run this because it will take a long time):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "chroma = librosa.feature.chroma_cqt(y=data_h, sr=sr)\n",
    "#plt.figure(figsize=(18,5))\n",
    "plt.figure(figsize=(20,8))\n",
    "librosa.display.specshow(chroma, sr=sr, x_axis='time', y_axis='chroma', vmin=0, vmax=1)\n",
    "#plt.title('Ara Hovanesyan v. Eric Holder, Jr. Chroma Spectrogram')\n",
    "plt.colorbar()\n",
    "#plt.title('Ara Hovanesyan v. Eric Holder, Jr.: Chroma Spectrogram')\n",
    "#librosa.display.specshow(chroma, sr=sr, x_axis='s', y_axis='chroma', );\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at just the first 30 seconds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_thirty_seconds = librosa.time_to_samples(30, sr=sr)\n",
    "intro = y[:first_thirty_seconds]\n",
    "intro_harm = librosa.effects.harmonic(intro)\n",
    "intro_chroma = librosa.feature.chroma_cqt(y=intro_harm, sr=sr)\n",
    "plt.figure(figsize=(20,8))\n",
    "#plt.title('U.S. v. Alfaro Chroma Spectrogram (First 30 seconds)')\n",
    "librosa.display.specshow(intro_chroma, sr=sr, x_axis='s', y_axis='chroma', )\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this plot we can see the pitch class and intensity across the first 30 seconds of the video. Again, while these are primarily useful for analyzing music, they can sometimes be helpful for speaker identification as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting it all together, we can extract all of these audio features with just a few lines of code. We can also extract the contrast (timber) of audio and tonnetz (tone):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sound feature engineering\n",
    "mel = librosa.feature.melspectrogram(y=y, sr=sr)\n",
    "mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40)\n",
    "chroma = librosa.feature.chroma_stft(S=mel, sr=sr)\n",
    "contrast = librosa.feature.spectral_contrast(S=mel, sr=sr)\n",
    "tonnetz = librosa.feature.tonnetz(y=librosa.effects.harmonic(y),sr=sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Challenge: Differences Between Speakers?\n",
    "\n",
    "Listen to the clip or watch the video. Try listening to the attorney speaking, then listen to the judges asking questions. Try clipping the soundfile to separate the attorney from the judges. Do their features look different at all? For example, people speak more or less harmonically or percussively? Louder or more quietly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Audio Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By themselves, audio features are not too helpful in a long video. However, the key is that by decomposing human voices into numerical features, they become usable by machine learning models. How does this work? Let's take a look at two clips, one of Judge Wardlaw speaking and one of Judge Fletcher. What do you notice about their mel spectrograms?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the audio file\n",
    "audio_path = \"data/audio_clips/wardlaw_seg1.wav\"\n",
    "y, sr = librosa.load(audio_path, res_type='kaiser_fast')\n",
    "\n",
    "# Generate the Mel spectrogram\n",
    "spec = librosa.feature.melspectrogram(y=y, sr=sr)\n",
    "\n",
    "# Display the spectrogram\n",
    "plt.figure(figsize=(10, 4))\n",
    "librosa.display.specshow(librosa.power_to_db(spec, ref=np.max), y_axis='mel', x_axis='time', sr=sr)\n",
    "plt.colorbar(format='%+2.0f dB')\n",
    "plt.title('Mel spectrogram - Wardlaw')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the audio file\n",
    "audio_path = \"data/audio_clips/Fletcher_seg1.wav\"\n",
    "y, sr = librosa.load(audio_path, res_type='kaiser_fast')\n",
    "\n",
    "# Generate the Mel spectrogram\n",
    "spec = librosa.feature.melspectrogram(y=y, sr=sr)\n",
    "\n",
    "# Display the spectrogram\n",
    "plt.figure(figsize=(10, 4))\n",
    "librosa.display.specshow(librosa.power_to_db(spec, ref=np.max), y_axis='mel', x_axis='time', sr=sr)\n",
    "plt.colorbar(format='%+2.0f dB')\n",
    "plt.title('Mel spectrogram - Fletcher')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A visual inspection suggests that Judge Wardlaw is a bit louder than Judge Fletcher, and speaks in a higher pitch. Can we train a machine learning model to uncover these kinds of patterns at scale? First let's set the path where our pre-labeled audio clips are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the path to the folder containing audio clips\n",
    "path = \"./data/audio_clips/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's define a function that extracts the mfcc's from an audio clip:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the audio files and extract features\n",
    "def extract_features(file):\n",
    "    X, sample_rate = librosa.load(file, res_type='kaiser_fast') \n",
    "    mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T,axis=0)\n",
    "    return mfccs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then get the mfcc's for each audio clip in our data and put them into a dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through the audio files and extract features\n",
    "data = []\n",
    "for subdir, dirs, files in os.walk(path):\n",
    "    for file in files:\n",
    "        try:\n",
    "            label = file.split(\"_\")[0]\n",
    "            file_path = os.path.join(subdir, file)\n",
    "            features = extract_features(file_path)\n",
    "            data.append([file_path, features, label])\n",
    "        except Exception as e:\n",
    "            print(\"Error encountered while parsing file: \", file)\n",
    "            print(e)\n",
    "\n",
    "# convert the data into a pandas dataframe\n",
    "df = pd.DataFrame(data, columns=['file_path', 'feature', 'label'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's go ahead and train a model. For audiovisual analysis, neural networks are oftentimes the best method available. We can start by doing our standard train/test splits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = df['label'].unique()\n",
    "\n",
    "# convert the labels into numerical values\n",
    "le = LabelEncoder()\n",
    "df['label'] = le.fit_transform(df['label'])\n",
    "\n",
    "# split the data into training and testing sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(np.array(df['feature'].tolist()), \n",
    "                                                    np.array(df['label'].tolist()), \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=42,\n",
    "                                                   stratify = np.array(df['label'].tolist()))\n",
    "\n",
    "# Convert labels to one-hot encoded vectors\n",
    "y_train_enc = to_categorical(y_train)\n",
    "y_val_enc = to_categorical(y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then set up our neural network. A neural network is a special type of supervised learning algorithm that simulates neurons in a brain to map inputs to outputs. While there is a lot to know about neural networks, here are a few key things:\n",
    "\n",
    "1. A neural net starts with an \"input\" layer that will have as many nodes as features in your dataset\n",
    "2. It will also have an \"output\" layer that will be the final prediction\n",
    "3. In between these two layers are \"hidden\" layers. We cannot see what goes on inside these hidden layers. At a high level, training a neural network involves multiple rounds (\"epochs\") where the model learns how to map the input layer to the output layer. After a pass through, the model sees where it made mistakes with its predictions, sends that information back through the network ('backpropagation'), and the hidden layers update accordingly. This process is known as \"deep learning.\"\n",
    "\n",
    "To set up a deep learning model to classify audio segments, we can set it up as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model architecture\n",
    "model = Sequential()\n",
    "# add the input layer\n",
    "model.add(Dense(256, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "# add a hidden layer\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "# add the output layer\n",
    "model.add(Dense(len(le.classes_), activation='softmax')) # changed y.unique() to le.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have defined the model parameters, we are ready to train it. What happens to our validation accuracy as we proceed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train_enc, validation_data=(X_val, y_val_enc), epochs=50, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    import itertools\n",
    "    fig = plt.figure(figsize=(8, 8)) \n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions on the test set\n",
    "y_pred = model.predict(X_val)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# create the confusion matrix with class names\n",
    "cm = confusion_matrix(y_val, y_pred_classes, labels=range(len(classes)), normalize=None)\n",
    "\n",
    "# create a DataFrame with the confusion matrix and class names\n",
    "#cm_df = pd.DataFrame(cm, index=classes, columns=classes)\n",
    "\n",
    "plot_confusion_matrix(cm, classes=classes,\n",
    "                      title='Confusion matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, that's really good! We can then save the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the trained model to a file\n",
    "model.save(\"/models/audio_classification_example.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional Challenge: Neural Networks\n",
    "\n",
    "We only used MFCCs and got great results, but this won't always be the case! Experiment with adding additional features to see how these performs as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transcription"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a way to classify speakers, we can start to analyze each speaker throughout a video. We can start with transcribing each speaker using OpenAI's `whisper` model. Before transcribing the video though, we first need to separate each speaker in a track. This is a process known as <b>diarization</b>. Diarization uses MFCCs to cluster speakers and separate them in an audio track. Luckily, this is easy to do with `pyannote`. First we initialize a diarization pipeline. If you haven't already, make sure you sign up for access at HuggingFace to get an authorization token [here](https://huggingface.co/pyannote/speaker-diarization-3.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"pyannote_token.txt\", \"r\") as file:\n",
    "    hg_token = file.read().replace(\"\\n\", \"\")\n",
    "\n",
    "pipeline = Pipeline.from_pretrained(\"pyannote/speaker-diarization-3.1\",\n",
    "                                    use_auth_token=hg_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how diarization works on the first minute of audio. Let's clip the audio file to the first minute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the .wav file\n",
    "input_file = \"11-71541 Ara Hovanesyan v. Eric Holder, Jr.-QEvMv81TIQM.wav\"\n",
    "audio = AudioSegment.from_wav(input_file)\n",
    "\n",
    "# Take the first five minutes (300 seconds)\n",
    "five_minute = 300 * 1000  # pydub works in milliseconds\n",
    "five_minute_audio = audio[:five_minute]\n",
    "\n",
    "# Export the first minute as a new .wav file\n",
    "output_file = \"output_five_minute.wav\"\n",
    "five_minute_audio.export(output_file, format=\"wav\")\n",
    "\n",
    "print(f\"The first minute of audio has been saved as {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we apply the diarization pipeline to the audio file, then print out the result. This code takes a while to run, so don't actually run it, the results have already been saved for you!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. apply pretrained pipeline\n",
    "diarization = pipeline(\"output_five_minute.wav\")\n",
    "\n",
    "# 5. print the result\n",
    "for turn, _, speaker in diarization.itertracks(yield_label=True):\n",
    "    print(f\"start={turn.start:.1f}s stop={turn.end:.1f}s speaker_{speaker}\")\n",
    "\n",
    "# dump the diarization output to disk using RTTM format\n",
    "#with open(\"output_five_minute.rttm\", \"w\") as rttm:\n",
    "#    diarization.write_rttm(rttm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's visualize the speaker turns. We can reload our `rttm` file that contains the diarization tracks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the RTTM file\n",
    "rttm_file_path = 'output_five_minute.rttm'\n",
    "\n",
    "# Function to read RTTM file and return an Annotation object\n",
    "def read_rttm(file_path):\n",
    "    annotation = Annotation()\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            fields = line.strip().split()\n",
    "            start_time = float(fields[3])\n",
    "            duration = float(fields[4])\n",
    "            speaker_id = fields[7]\n",
    "            segment = Segment(start_time, start_time + duration)\n",
    "            annotation[segment] = speaker_id\n",
    "    return annotation\n",
    "\n",
    "# Read the RTTM file\n",
    "diarization = read_rttm(rttm_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then segment as necessary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the diarization result\n",
    "notebook.crop = Segment(0, 300) \n",
    "fig, ax = plt.subplots()\n",
    "notebook.plot_annotation(diarization, ax=ax, time=True, legend=True)\n",
    "plt.show()\n",
    "\n",
    "# Print the contents for inspection\n",
    "for segment, _, speaker in diarization.itertracks(yield_label=True):\n",
    "    print(f\"Speaker: {speaker}, Start: {segment.start:.2f}, End: {segment.end:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the speaker turns in the overall audio track. What do you notice about who is speaking the most in the first and second half of the audio? We won't run the code to create the diarization because this would take a while, so let's skip right to looking at the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# apply pretrained pipeline\n",
    "diarization = pipeline(\"audio.wav\")\n",
    "\n",
    "# dump the diarization output to disk using RTTM format\n",
    "with open(\"audio.rttm\", \"w\") as rttm:\n",
    "    diarization.write_rttm(rttm)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the RTTM file\n",
    "diarization = read_rttm('audio.rttm')\n",
    "\n",
    "# create plot\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "# define color map\n",
    "color_map = {\n",
    "    \"SPEAKER_04\": \"blue\",\n",
    "    \"SPEAKER_00\": \"green\",\n",
    "    \"SPEAKER_01\": \"red\",\n",
    "    \"SPEAKER_03\": \"purple\",\n",
    "    \"SPEAKER_02\": \"orange\",\n",
    "}\n",
    "\n",
    "# plot speaker turns\n",
    "for turn, _, speaker in diarization.itertracks(yield_label=True):\n",
    "    start = turn.start\n",
    "    end = turn.end\n",
    "    duration = end - start\n",
    "    ax.barh(y=speaker, width=duration, left=start, height=0.5, color=color_map[speaker])\n",
    "\n",
    "# set plot labels and title\n",
    "ax.set_xlabel(\"Time (seconds)\")\n",
    "ax.set_ylabel(\"Speaker\")\n",
    "ax.set_title(\"Speaker turns\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Speaker Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now ready to classify each speaker segment! First let's load the neural network we trained earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. load saved model\n",
    "model = tf.keras.models.load_model(\"/models/audio_classification_example.h5\")\n",
    "\n",
    "def extract_features(file):\n",
    "    X, sample_rate = librosa.load(file, res_type='kaiser_fast') \n",
    "    mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T,axis=0)\n",
    "    return mfccs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's extract the MFCCs for each speaker:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_times = []\n",
    "end_times = []\n",
    "speakers = []\n",
    "features = []\n",
    "\n",
    "audio_file, sample_rate = librosa.load('audio.wav', res_type='kaiser_fast')\n",
    "\n",
    "for turn, _, speaker in diarization.itertracks(yield_label=True):\n",
    "\n",
    "    # Get the start and end times of the track\n",
    "    start_times.append(turn.start)\n",
    "    end_times.append(turn.end)\n",
    "    speakers.append(speaker)\n",
    "\n",
    "# Loop through each segment and extract the audio data\n",
    "for i, (start_time, end_time) in enumerate(zip(start_times, end_times)):\n",
    "    start_frame = int(start_time * sample_rate)\n",
    "    end_frame = int(end_time * sample_rate)\n",
    "    segment = audio_file[start_frame:end_frame]\n",
    "    mfccs = np.mean(librosa.feature.mfcc(y=segment, sr=sample_rate, n_mfcc=40).T,axis=0)\n",
    "    features.append(mfccs)\n",
    "    #librosa.output.write_wav(f\"segment_{i+1}.wav\", segment, sr)\n",
    "\n",
    "clips = pd.DataFrame({'start': start_times, 'end': end_times,\n",
    "                     'features': features,\n",
    "                     'speakers': speakers})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then predict the speaker for each track, using a .9 confidence threshold:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_audio = np.array(clips['features'].tolist())\n",
    "\n",
    "# Predict the classes for the input data\n",
    "y_pred_prob = model.predict(processed_audio)\n",
    "\n",
    "# Define the threshold probability below which the predicted class will be changed\n",
    "threshold = 0.9\n",
    "\n",
    "y_preds = []\n",
    "# Loop through each predicted probability vector\n",
    "for i in range(len(y_pred_prob)):\n",
    "    \n",
    "    # Get the predicted class and probability\n",
    "    y_pred = np.argmax(y_pred_prob[i])\n",
    "    prob = y_pred_prob[i][y_pred]\n",
    "    \n",
    "    # If the predicted probability is below the threshold, predict a different class\n",
    "    if prob < threshold:\n",
    "        y_pred = 9  # predict class 9 instead\n",
    "    \n",
    "    y_preds.append(y_pred)\n",
    "\n",
    "clips['speaker_pred'] = y_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot it! We know that that the two most common speakers are probably the attorneys so we can take them out of our classification and then plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the mapping dictionary\n",
    "mapping = {0: 'Fletcher', 2: 'Owens', 3: 'Wardlaw', 9: 'unknown'}\n",
    "\n",
    "# Apply the mapping to the 'values' column\n",
    "clips['speaker_pred'] = clips['speaker_pred'].map(mapping)\n",
    "\n",
    "value_counts = clips['speakers'].value_counts()\n",
    "# Get the two most common values\n",
    "most_common = value_counts.nlargest(2)\n",
    "\n",
    "mask = clips['speakers'].isin(['SPEAKER_00', 'SPEAKER_01'])\n",
    "clips.loc[mask, 'speaker_pred'] = 'attorney'\n",
    "clips['speaker_pred'] = clips['speaker_pred'].fillna('unknown')\n",
    "\n",
    "# create plot\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "# define color map\n",
    "color_map = {\n",
    "    \"attorney\": \"blue\",\n",
    "    \"Owens\": \"red\",\n",
    "    \"Fletcher\": \"purple\",\n",
    "    \"Wardlaw\": \"orange\",\n",
    "    'unknown': 'gray',\n",
    "}\n",
    "\n",
    "# plot speaker turns\n",
    "for start, end, speaker in zip(clips['start'],\n",
    "                               clips['end'],\n",
    "                               clips['speaker_pred']):\n",
    "    start = start\n",
    "    end = end\n",
    "    duration = end - start\n",
    "    ax.barh(y=speaker, width=duration, left=start, height=0.5, color=color_map[speaker])\n",
    "\n",
    "# set plot labels and title\n",
    "ax.set_xlabel(\"Time (seconds)\")\n",
    "ax.set_ylabel(\"Speaker\")\n",
    "ax.set_title(\"Speaker turns\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transcribe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to create a transcript! First let's load the `whisper` transcription model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = whisper.load_model(\"base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then loop through our speaker segments and transcribe each part. Let's see how this works on a short sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clips[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for start, end in zip(clips['start'][0:2],\n",
    "                      clips['end'][0:2]):\n",
    "    start_frame = int(start * sample_rate)\n",
    "    end_frame = int(end * sample_rate)\n",
    "    segment = audio_file[start_frame:end_frame]\n",
    "    #result = model.transcribe(segment)\n",
    "    sf.write('temp.wav', segment, 48000, 'PCM_24')\n",
    "    result = model.transcribe(\"temp.wav\")\n",
    "    results.append(result['text'])\n",
    "    \n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That doesn't seem great - why not? `whiper` works better with longer audio tracks! This is an important limitation to keep in mind in your own projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "results = []\n",
    "for start, end, speaker in zip(clips['start'],\n",
    "                               clips['end'],\n",
    "                               clips['speaker_pred']):\n",
    "    start_frame = int(start * sample_rate)\n",
    "    end_frame = int(end * sample_rate)\n",
    "    segment = audio_file[start_frame:end_frame]\n",
    "    result = model.transcribe(segment)\n",
    "    results.append(result)\n",
    "    #librosa.output.write_wav('temp.wav', segment, sr)\n",
    "    sf.write('temp.wav', segment, 48000, 'PCM_24')\n",
    "    result = model.transcribe(\"temp.wav\")\n",
    "    results.append(result)\n",
    "    \n",
    "clips['transcription'] = results\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively we could transcribe the entire audio file first, and then segment it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new dataframe with simplified rows\n",
    "new_df = pd.DataFrame(columns=['start', 'end', 'speaker_pred'])\n",
    "i = 0\n",
    "while i < len(clips):\n",
    "    # get speaker and start time of the current row\n",
    "    speaker_pred = clips.loc[i, 'speaker_pred']\n",
    "    start_time = clips.loc[i, 'start']\n",
    "\n",
    "    # find the index of the last row with the same speaker\n",
    "    j = i + 1\n",
    "    while j < len(clips) and clips.loc[j, 'speaker_pred'] == speaker_pred:\n",
    "        j += 1\n",
    "\n",
    "    # get the end time of the last row with the same speaker\n",
    "    end_time = clips.loc[j - 1, 'end']\n",
    "\n",
    "    # add the simplified row to the new dataframe\n",
    "    new_df = new_df.append({'start': start_time, 'end': end_time, 'speaker_pred': speaker_pred},\n",
    "                           ignore_index=True)\n",
    "\n",
    "    # move to the next row with a different speaker\n",
    "    i = j\n",
    "\n",
    "print(new_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go ahead and transcribe the first 5 minutes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_transcript = model.transcribe('output_five_minute.wav')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then match the transcript segments to our diarization timestamps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for segment in full_transcript['segments']:\n",
    "    # initialize speaker to None\n",
    "    speaker_pred = None\n",
    "    \n",
    "    # iterate over the simplified rows in new_df\n",
    "    for _, row in new_df.iterrows():\n",
    "        # if clip's end time is less than or equal to a row's end time\n",
    "        if segment['end'] <= row['end']:\n",
    "            # set speaker to the row's speaker value\n",
    "            speaker_pred = row['speaker_pred']\n",
    "            break\n",
    "    \n",
    "    # add speaker key to clip dictionary\n",
    "    segment['speaker_pred'] = speaker_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what our transcript looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in full_transcript['segments']:\n",
    "    print(f\"start: {d['start']}, end: {d['end']}, speaker_pred: {d['speaker_pred']}, text: {d['text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Listen to the video. How does the transcript match up to the actual audio? Do we capture who is talking when, and what they're saying exactly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/Ara Hovanesyan v. Eric Holder.txt', 'w') as f:\n",
    "    for d in full_transcript['segments']:\n",
    "        f.write(f\"start: {d['start']}, end: {d['end']}, speaker_pred: {d['speaker_pred']}, text: {d['text']} '\\n'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame\n",
    "df = pd.DataFrame(full_transcript['segments'])\n",
    "\n",
    "# Select the relevant columns\n",
    "df_selected = df[['start', 'end', 'text', 'speaker_pred']]\n",
    "\n",
    "# Display the DataFrame\n",
    "df_selected.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "full_transcript_all = model.transcribe('audio.wav')\n",
    "with open('./data/Ara Hovanesyan v. Eric Holder full.txt', 'w') as f:\n",
    "    for d in full_transcript['segments']:\n",
    "        f.write(f\"start: {d['start']}, end: {d['end']}, speaker_pred: {d['speaker_pred']}, text: {d['text']} '\\n'\")\n",
    "\n",
    "for segment in full_transcript_all['segments']:\n",
    "    # initialize speaker to None\n",
    "    speaker_pred = None\n",
    "    \n",
    "    # iterate over the simplified rows in new_df\n",
    "    for _, row in new_df.iterrows():\n",
    "        # if clip's end time is less than or equal to a row's end time\n",
    "        if segment['end'] <= row['end']:\n",
    "            # set speaker to the row's speaker value\n",
    "            speaker_pred = row['speaker_pred']\n",
    "            break\n",
    "    \n",
    "    # add speaker key to clip dictionary\n",
    "    segment['speaker_pred'] = speaker_pred\n",
    "    \n",
    "# Create a DataFrame\n",
    "df_all = pd.DataFrame(full_transcript_all['segments'])\n",
    "\n",
    "# Select the relevant columns\n",
    "df_all_selected = df_all[['start', 'end', 'text', 'speaker_pred']]\n",
    "\n",
    "# Display the DataFrame\n",
    "df_all_selected.to_csv('Ara Hovanesyan v. Eric Holder full transcript.csv')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Challenge: Text Analysis of Transcripts\n",
    "\n",
    "Using some of the text analysis techniques that we covered earlier this week, see if you can explore any interesting insights about the transcript! Do different speakers have different sentiments? Do the two different attorneys have different common words or different sentiments?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_selected = pd.read_csv('Ara Hovanesyan v. Eric Holder full transcript.csv')\n",
    "df_all_selected.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computer Vision Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we're done with audio analysis, let's turn to video. The main difference between computational audio analysis and computer vision is that computer vision uses a <i>frame</i> as its basic unit. Videos are essentially lots of pictures stacked one after another. We therefore start with one picture, and then scale up to an entire video. Let's start by extracting the first frame from our video:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the video file\n",
    "video_path = \"11-71541 Ara Hovanesyan v. Eric Holder, Jr.-QEvMv81TIQM.mp4\"\n",
    "# Path to save the extracted frame\n",
    "output_image_path = 'first_frame.jpg'\n",
    "\n",
    "# Frame number to extract\n",
    "frame_number = 0\n",
    "\n",
    "# Open the video file\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Check if the video was opened successfully\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open video.\")\n",
    "else:\n",
    "    # Set the video position to the desired frame number\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, frame_number)\n",
    "    \n",
    "    # Read the first frame\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if ret:\n",
    "        # Convert the frame from BGR to RGB format\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Convert the frame to a PIL Image\n",
    "        image = Image.fromarray(frame_rgb)\n",
    "        \n",
    "        # Save the image\n",
    "        image.save(output_image_path)\n",
    "        print(f\"First frame saved as {output_image_path}\")\n",
    "        \n",
    "        # Display the image (optional)\n",
    "        image.show()\n",
    "    else:\n",
    "        print(\"Error: Could not read frame.\")\n",
    "\n",
    "    # Release the video capture object\n",
    "    cap.release()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Face Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to detect the human faces in a frame. We will use pre-trained face detection models to do this. These are pre-trained Haar Cascades models that are specialized in finding faces in an image. We can load them like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the image\n",
    "image_path = \"first_frame.jpg\"\n",
    "image = cv2.imread(image_path)\n",
    "\n",
    "# Get the dimensions of the image\n",
    "(h, w) = image.shape[:2]\n",
    "\n",
    "# Crop the image to the top half\n",
    "top_half = image[:h//2, :]\n",
    "\n",
    "# Load the Haar Cascade for face detection\n",
    "haar_cascade_path = cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\"\n",
    "face_cascade = cv2.CascadeClassifier(haar_cascade_path)\n",
    "\n",
    "# Convert the cropped image to grayscale\n",
    "gray_top_half = cv2.cvtColor(top_half, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Perform face detection\n",
    "faces = face_cascade.detectMultiScale(gray_top_half, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30), flags=cv2.CASCADE_SCALE_IMAGE)\n",
    "\n",
    "# Draw bounding boxes around detected faces\n",
    "for (x, y, w, h) in faces:\n",
    "    cv2.rectangle(top_half, (x, y), (x + w, y + h), (0, 0, 255), 2)\n",
    "\n",
    "# Display the output\n",
    "plt.imshow(cv2.cvtColor(top_half, cv2.COLOR_BGR2RGB))\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "# Save the output image with bounding boxes\n",
    "output_path = 'output_image.jpg'\n",
    "\n",
    "# Check the number of faces detected\n",
    "num_faces_detected = len(faces)\n",
    "num_faces_detected\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Face Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have detected the faces, we need to identify <i>who</i> is in each bounding box. There are different options for training a model - here we will use a combination of a neural network and a Support Vector Classifier. The neural network will detect faces in our training set, and the SVC model will be trained to detect faces in our actual videos.\n",
    "\n",
    "First let's split the \"patches\" directory that contains all of our images into a training and validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(10)\n",
    "\n",
    "# The path to the folder containing the subfolders of images\n",
    "folder_path = './patches/'\n",
    "\n",
    "# The percentage of images to use for validation (e.g. 10% = 0.1)\n",
    "validation_percentage = 0.1\n",
    "\n",
    "# Create a list of the subfolders in the folder\n",
    "subfolders = [f.name for f in os.scandir(folder_path) if f.is_dir()]\n",
    "\n",
    "# Create a dictionary to map the subfolder names to their corresponding label\n",
    "labels = {name: i for i, name in enumerate(subfolders)}\n",
    "\n",
    "# Create empty lists for the training and validation images and labels\n",
    "train_images = []\n",
    "train_labels = []\n",
    "validation_images = []\n",
    "validation_labels = []\n",
    "\n",
    "# Iterate over the subfolders\n",
    "for subfolder in subfolders:\n",
    "    # Get the path to the subfolder\n",
    "    subfolder_path = os.path.join(folder_path, subfolder)\n",
    "\n",
    "    # Get the list of image files in the subfolder\n",
    "    image_files = [f.name for f in os.scandir(subfolder_path) if f.is_file()]\n",
    "\n",
    "    # Shuffle the list of image files\n",
    "    random.shuffle(image_files)\n",
    "\n",
    "    # Get the number of images to use for validation\n",
    "    num_validation_images = int(len(image_files) * validation_percentage)\n",
    "\n",
    "    # Split the images into training and validation sets\n",
    "    validation_images += [os.path.join(subfolder_path, f) for f in image_files[:num_validation_images]]\n",
    "    train_images += [os.path.join(subfolder_path, f) for f in image_files[num_validation_images:]]\n",
    "\n",
    "    # Create the corresponding labels for the training and validation sets\n",
    "    validation_labels += [labels[subfolder]] * num_validation_images\n",
    "    train_labels += [labels[subfolder]] * (len(image_files) - num_validation_images)\n",
    "\n",
    "# Print the number of images in the training and validation sets\n",
    "print(f'Number of training images: {len(train_images)}')\n",
    "print(f'Number of validation images: {len(validation_images)}')\n",
    "\n",
    "# The list of file paths to copy\n",
    "#file_paths = ['path/to/file1', 'path/to/file2', 'path/to/file3']\n",
    "\n",
    "# The path to the new folder\n",
    "train_folder_path = './train'\n",
    "\n",
    "# Create the new folder if it doesn't already exist\n",
    "if not os.path.exists(train_folder_path):\n",
    "    os.makedirs(train_folder_path)\n",
    "\n",
    "# Iterate over the list of file paths\n",
    "for file_path in train_images:\n",
    "    # Get the directory of the file\n",
    "    dir_path = os.path.dirname(file_path)\n",
    "\n",
    "    # Create the directory in the new folder if it doesn't already exist\n",
    "    dest_dir_path = os.path.join(train_folder_path, dir_path)\n",
    "    if not os.path.exists(dest_dir_path):\n",
    "        os.makedirs(dest_dir_path)\n",
    "\n",
    "    # Copy the file to the new folder\n",
    "    shutil.copy(file_path, dest_dir_path)\n",
    "    \n",
    "# The path to the new folder\n",
    "val_folder_path = './val'\n",
    "\n",
    "# Create the new folder if it doesn't already exist\n",
    "if not os.path.exists(val_folder_path):\n",
    "    os.makedirs(val_folder_path)\n",
    "\n",
    "# Iterate over the list of file paths\n",
    "for file_path in validation_images:\n",
    "    # Get the directory of the file\n",
    "    dir_path = os.path.dirname(file_path)\n",
    "\n",
    "    # Create the directory in the new folder if it doesn't already exist\n",
    "    dest_dir_path = os.path.join(val_folder_path, dir_path)\n",
    "    if not os.path.exists(dest_dir_path):\n",
    "        os.makedirs(dest_dir_path)\n",
    "\n",
    "    # Copy the file to the new folder\n",
    "    shutil.copy(file_path, dest_dir_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train a SVC model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_recognition_model(train_dir='train'):\n",
    "    # load our serialized face detector from disk\n",
    "    print(\"[INFO] loading face detector...\")\n",
    "    protoPath = \"./recognizers/face_detection_model/deploy.prototxt\"\n",
    "    modelPath = \"./recognizers/face_detection_model/res10_300x300_ssd_iter_140000.caffemodel\"\n",
    "    detector = cv2.dnn.readNetFromCaffe(protoPath, modelPath)\n",
    "    embedder = cv2.dnn.readNetFromTorch('./recognizers/openface.nn4.small2.v1.t7')\n",
    "\n",
    "    print(\"[INFO] quantifying faces...\")\n",
    "    imagePaths = list(paths.list_images(train_dir))\n",
    "    knownEmbeddings = []\n",
    "    knownNames = []\n",
    "    total = 0\n",
    "\n",
    "    for (i, imagePath) in enumerate(imagePaths):\n",
    "        # extract the person name from the image path\n",
    "        name = imagePath.split(os.path.sep)[-2]\n",
    "\n",
    "        # load the image and get its dimensions\n",
    "        image = cv2.imread(imagePath)\n",
    "        (h, w) = image.shape[:2]\n",
    "\n",
    "        # construct a blob from the image\n",
    "        imageBlob = cv2.dnn.blobFromImage(\n",
    "            cv2.resize(image, (300, 300)), 1.0, (300, 300),\n",
    "            (104.0, 177.0, 123.0), swapRB=False, crop=False)\n",
    "\n",
    "        # apply OpenCV's deep learning-based face detector to localize faces in the input image\n",
    "        detector.setInput(imageBlob)\n",
    "        detections = detector.forward()\n",
    "\n",
    "        # ensure at least one face was found\n",
    "        if len(detections) > 0:\n",
    "            # find the detection with the largest probability\n",
    "            i = np.argmax(detections[0, 0, :, 2])\n",
    "            confidence = detections[0, 0, i, 2]\n",
    "\n",
    "            # ensure that the detection meets the minimum confidence threshold\n",
    "            if confidence > .85:\n",
    "                # compute the (x, y)-coordinates of the bounding box for the face\n",
    "                box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "                (startX, startY, endX, endY) = box.astype(\"int\")\n",
    "\n",
    "                # extract the face ROI and grab the ROI dimensions\n",
    "                face = image[startY:endY, startX:endX]\n",
    "                (fH, fW) = face.shape[:2]\n",
    "\n",
    "                # ensure the face width and height are sufficiently large\n",
    "                if fW < 20 or fH < 20:\n",
    "                    continue\n",
    "\n",
    "                # construct a blob for the face ROI, then pass the blob through our face embedding model to obtain the 128-d quantification of the face\n",
    "                faceBlob = cv2.dnn.blobFromImage(face, 1.0 / 255,\n",
    "                    (96, 96), (0, 0, 0), swapRB=True, crop=False)\n",
    "                embedder.setInput(faceBlob)\n",
    "                vec = embedder.forward()\n",
    "\n",
    "                # add the name of the person + corresponding face embedding to their respective lists\n",
    "                knownNames.append(name)\n",
    "                knownEmbeddings.append(vec.flatten())\n",
    "                total += 1\n",
    "\n",
    "    # serialize the facial embeddings + names to disk\n",
    "    print(\"[INFO] serializing {} encodings...\".format(total))\n",
    "    data = {\"embeddings\": knownEmbeddings, \"names\": knownNames}\n",
    "    with open('./recognizers/embeddings.pickle', \"wb\") as f:\n",
    "        f.write(pickle.dumps(data))\n",
    "\n",
    "    # load the face embeddings\n",
    "    print(\"[INFO] loading face embeddings...\")\n",
    "    data = pickle.loads(open('./recognizers/embeddings.pickle', \"rb\").read())\n",
    "    le = LabelEncoder()\n",
    "    labels = le.fit_transform(data[\"names\"])\n",
    "\n",
    "    # train the model using GridSearchCV to find the best parameters\n",
    "    print(\"[INFO] training model with GridSearchCV...\")\n",
    "    param_grid = {'C': [0.1, 1, 10, 100], 'kernel': ['linear', 'rbf']}\n",
    "    model = GridSearchCV(SVC(probability=True), param_grid, cv=10)\n",
    "    model.fit(data[\"embeddings\"], labels)\n",
    "\n",
    "    # save the best model\n",
    "    print(f\"[INFO] best model parameters: {model.best_params_}\")\n",
    "    recognizer = model.best_estimator_\n",
    "\n",
    "    # write the actual face recognition model to disk\n",
    "    with open('./recognizers/recognizer.pickle', \"wb\") as f:\n",
    "        f.write(pickle.dumps(recognizer))\n",
    "\n",
    "    # write the label encoder to disk\n",
    "    with open('./recognizers/le.pickle', \"wb\") as f:\n",
    "        f.write(pickle.dumps(le))\n",
    "\n",
    "    return recognizer, le\n",
    "\n",
    "# Now call this function before entering the loop\n",
    "recognizer, le = train_recognition_model()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's see how we did:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(val_dir='val', recognizer=None, le=None):\n",
    "    # load our serialized face detector from disk\n",
    "    print(\"[INFO] loading face detector...\")\n",
    "    protoPath = \"./recognizers/face_detection_model/deploy.prototxt\"\n",
    "    modelPath = \"./recognizers/face_detection_model/res10_300x300_ssd_iter_140000.caffemodel\"\n",
    "    detector = cv2.dnn.readNetFromCaffe(protoPath, modelPath)\n",
    "    embedder = cv2.dnn.readNetFromTorch('./recognizers/openface.nn4.small2.v1.t7')\n",
    "\n",
    "    imagePaths = list(paths.list_images(val_dir))\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "\n",
    "    for imagePath in imagePaths:\n",
    "        # Extract the person name from the image path\n",
    "        name = imagePath.split(os.path.sep)[-2]\n",
    "        \n",
    "        # Load the image and get its dimensions\n",
    "        image = cv2.imread(imagePath)\n",
    "        (h, w) = image.shape[:2]\n",
    "\n",
    "        # Construct a blob from the image\n",
    "        imageBlob = cv2.dnn.blobFromImage(\n",
    "            cv2.resize(image, (300, 300)), 1.0, (300, 300),\n",
    "            (104.0, 177.0, 123.0), swapRB=False, crop=False)\n",
    "\n",
    "        # Apply OpenCV's deep learning-based face detector to localize faces in the input image\n",
    "        detector.setInput(imageBlob)\n",
    "        detections = detector.forward()\n",
    "\n",
    "        # Ensure at least one face was found\n",
    "        if len(detections) > 0:\n",
    "            # Find the detection with the largest probability\n",
    "            i = np.argmax(detections[0, 0, :, 2])\n",
    "            confidence = detections[0, 0, i, 2]\n",
    "\n",
    "            # Ensure that the detection with the largest probability also\n",
    "            # meets our minimum probability test (thus helping filter out\n",
    "            # weak detections)\n",
    "            if confidence > .85:\n",
    "                # Compute the (x, y)-coordinates of the bounding box for the face\n",
    "                box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "                (startX, startY, endX, endY) = box.astype(\"int\")\n",
    "\n",
    "                # Extract the face ROI and grab the ROI dimensions\n",
    "                face = image[startY:endY, startX:endX]\n",
    "                (fH, fW) = face.shape[:2]\n",
    "\n",
    "                # Ensure the face width and height are sufficiently large\n",
    "                if fW < 20 or fH < 20:\n",
    "                    continue\n",
    "\n",
    "                # Construct a blob for the face ROI, then pass the blob\n",
    "                # through our face embedding model to obtain the 128-d\n",
    "                # quantification of the face\n",
    "                faceBlob = cv2.dnn.blobFromImage(face, 1.0 / 255,\n",
    "                    (96, 96), (0, 0, 0), swapRB=True, crop=False)\n",
    "                embedder.setInput(faceBlob)\n",
    "                vec = embedder.forward()\n",
    "\n",
    "                # Classify faces with most likely prediction\n",
    "                preds = recognizer.predict_proba(vec)[0]\n",
    "                j = np.argmax(preds)\n",
    "                predicted_name = le.classes_[j]\n",
    "\n",
    "                true_labels.append(name)\n",
    "                predicted_labels.append(predicted_name)\n",
    "\n",
    "    # Convert true and predicted labels to encoded labels\n",
    "    true_labels_encoded = le.transform(true_labels)\n",
    "    predicted_labels_encoded = le.transform(predicted_labels)\n",
    "\n",
    "    # Generate confusion matrix\n",
    "    cm = confusion_matrix(true_labels_encoded, predicted_labels_encoded)\n",
    "    report = classification_report(true_labels_encoded, predicted_labels_encoded, target_names=le.classes_)\n",
    "\n",
    "    # Display the confusion matrix\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=le.classes_, yticklabels=le.classes_)\n",
    "    plt.ylabel('Actual')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Classification Report:\\n\", report)\n",
    "\n",
    "# Evaluate the model using the validation set\n",
    "evaluate_model(val_dir='val', recognizer=recognizer, le=le)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is pretty good! Though we can already see some issues that might occur as we scale - a relatively low recall for Judge Owens and a relatively low precision for Judge Fletcher for example. In actuality, we would train with many more images to make boost these numbers, but this takes time so we will not do this for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Face Emotion Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know who is in the frame, we can start extracting other features about each judge. One thing we might be interested in is judges' emotional states throughout a video. Are they happy or sad? Bored? Angry? One way to do this might be to train a custom neural network to identify these emotions. For now, we will use another pre-trained model though. Here we will use the `FER` library: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put in the location of the video file that has to be processed\n",
    "location_videofile = \"11-71541 Ara Hovanesyan v. Eric Holder, Jr.-QEvMv81TIQM.mp4\"\n",
    "\n",
    "# Build the Face detection detector\n",
    "face_detector = FER(mtcnn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the image using OpenCV\n",
    "image_path = \"first_frame.jpg\"\n",
    "image = cv2.imread(image_path)\n",
    "\n",
    "# Initialize the FER detector\n",
    "detector = FER(mtcnn=True)\n",
    "\n",
    "# Detect emotions in the image\n",
    "emotions = detector.detect_emotions(image)\n",
    "\n",
    "# Draw bounding boxes and emotions on the image\n",
    "for emotion in emotions:\n",
    "    box = emotion[\"box\"]\n",
    "    (x, y, w, h) = box\n",
    "    cv2.rectangle(image, (x, y), (x + w, y + h), (255, 0, 0), 2)\n",
    "    \n",
    "    emotion_text = max(emotion[\"emotions\"], key=emotion[\"emotions\"].get)\n",
    "    cv2.putText(image, emotion_text, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 0, 0), 2)\n",
    "\n",
    "# Convert the image from BGR to RGB format\n",
    "image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Display the image with matplotlib\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(image_rgb)\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Body language also conveys information about how judges may be reacting to what is going on in the courtroom. There are several models that are useful for this task, but many such as `AlphaPose` and `OpenPose` require some difficult installations. We will use a basic version of `OpenPose` that detects individual body parts and records their positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the body parts and pose pairs for visualization\n",
    "BODY_PARTS = { \"Nose\": 0, \"Neck\": 1, \"RShoulder\": 2, \"RElbow\": 3, \"RWrist\": 4,\n",
    "               \"LShoulder\": 5, \"LElbow\": 6, \"LWrist\": 7, \"RHip\": 8, \"RKnee\": 9,\n",
    "               \"RAnkle\": 10, \"LHip\": 11, \"LKnee\": 12, \"LAnkle\": 13, \"REye\": 14,\n",
    "               \"LEye\": 15, \"REar\": 16, \"LEar\": 17, \"Background\": 18 }\n",
    "\n",
    "POSE_PAIRS = [ [\"Neck\", \"RShoulder\"], [\"Neck\", \"LShoulder\"], [\"RShoulder\", \"RElbow\"],\n",
    "               [\"RElbow\", \"RWrist\"], [\"LShoulder\", \"LElbow\"], [\"LElbow\", \"LWrist\"],\n",
    "               [\"Neck\", \"RHip\"], [\"RHip\", \"RKnee\"], [\"RKnee\", \"RAnkle\"], [\"Neck\", \"LHip\"],\n",
    "               [\"LHip\", \"LKnee\"], [\"LKnee\", \"LAnkle\"], [\"Neck\", \"Nose\"], [\"Nose\", \"REye\"],\n",
    "               [\"REye\", \"REar\"], [\"Nose\", \"LEye\"], [\"LEye\", \"LEar\"] ]\n",
    "\n",
    "# Load the OpenPose model\n",
    "net = cv.dnn.readNetFromTensorflow(\"recognizers/graph_opt.pb\")\n",
    "\n",
    "# Load the MobileNet-SSD model\n",
    "ssd_net = cv.dnn.readNetFromCaffe(\"recognizers/body_detection_model/deploy.prototxt\", \n",
    "                                  \"recognizers/body_detection_model/mobilenet_iter_73000.caffemodel\")\n",
    "\n",
    "# Parameters\n",
    "image_width = 600\n",
    "image_height = 600\n",
    "threshold = 0.1  # Lowered threshold to capture more keypoints\n",
    "\n",
    "# Load the image\n",
    "img = cv.imread('first_frame.jpg', cv.IMREAD_UNCHANGED)\n",
    "photo_height, photo_width = img.shape[:2]\n",
    "\n",
    "# Detect persons using MobileNet-SSD\n",
    "blob = cv.dnn.blobFromImage(img, 0.007843, (300, 300), 127.5)\n",
    "ssd_net.setInput(blob)\n",
    "detections = ssd_net.forward()\n",
    "\n",
    "print(f\"Detected {detections.shape[2]} objects\")\n",
    "\n",
    "# Loop over the detections\n",
    "for i in range(detections.shape[2]):\n",
    "    confidence = detections[0, 0, i, 2]\n",
    "    if confidence > 0.7:\n",
    "        class_id = int(detections[0, 0, i, 1])\n",
    "        # Check if the detected object is a person (class_id == 15 for MobileNet-SSD)\n",
    "        if class_id == 15:\n",
    "            x1 = int(detections[0, 0, i, 3] * photo_width)\n",
    "            y1 = int(detections[0, 0, i, 4] * photo_height)\n",
    "            x2 = int(detections[0, 0, i, 5] * photo_width)\n",
    "            y2 = int(detections[0, 0, i, 6] * photo_height)\n",
    "            cv.rectangle(img, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
    "            print(f\"Person detected at x1: {x1}, y1: {y1}, x2: {x2}, y2: {y2}\")\n",
    "\n",
    "            # Process the detected person region with OpenPose\n",
    "            body_roi = img[y1:y2, x1:x2]\n",
    "            body_blob = cv.dnn.blobFromImage(body_roi, 1.0, (image_width, image_height), (127.5, 127.5, 127.5), swapRB=True, crop=False)\n",
    "            net.setInput(body_blob)\n",
    "            out = net.forward()\n",
    "            out = out[:, :19, :, :]\n",
    "\n",
    "            points = []\n",
    "            for j in range(len(BODY_PARTS)):\n",
    "                heatMap = out[0, j, :, :]\n",
    "                _, conf, _, point = cv.minMaxLoc(heatMap)\n",
    "                x_coord = (point[0] * (x2 - x1)) / out.shape[3] + x1\n",
    "                y_coord = (point[1] * (y2 - y1)) / out.shape[2] + y1\n",
    "                if conf > threshold:\n",
    "                    points.append((int(x_coord), int(y_coord)))\n",
    "                    cv.ellipse(img, (int(x_coord), int(y_coord)), (5, 5), 0, 0, 360, (0, 255, 255), cv.FILLED)\n",
    "                else:\n",
    "                    points.append(None)\n",
    "\n",
    "            for pair in POSE_PAIRS:\n",
    "                partFrom = pair[0]\n",
    "                partTo = pair[1]\n",
    "                idFrom = BODY_PARTS[partFrom]\n",
    "                idTo = BODY_PARTS[partTo]\n",
    "\n",
    "                if points[idFrom] and points[idTo]:\n",
    "                    cv.line(img, points[idFrom], points[idTo], (0, 255, 0), 3)\n",
    "                    cv.ellipse(img, points[idFrom], (3, 3), 0, 0, 360, (0, 0, 255), cv.FILLED)\n",
    "                    cv.ellipse(img, points[idTo], (3, 3), 0, 0, 360, (0, 0, 255), cv.FILLED)\n",
    "\n",
    "# Display the image\n",
    "cv.imshow(\"Pose Estimation\", img)\n",
    "cv.waitKey(0)\n",
    "cv.destroyAllWindows()\n",
    "\n",
    "# Save the image\n",
    "cv.imwrite(\"openpose_example.png\", img)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it All Together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to put it all together! We can take our code so far and loop through each frame in a video to build a dataset. One way to organize such a large amount of data is in a Python dictionary. A dictionary is structured as a series of key-value pairs. For example, the key can be the frame number, and then the values can be information about each judge. Even better, the value can be another dictionary! So we can make keys for each judge, keys corresponding to each of our measures (emotion, body language, etc.), and values corresponding to each. Let's see how we would do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Haar Cascade for face detection\n",
    "haar_cascade_path = cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\"\n",
    "face_cascade = cv2.CascadeClassifier(haar_cascade_path)\n",
    "\n",
    "# Load the face recognition model\n",
    "embedder = cv2.dnn.readNetFromTorch('./recognizers/openface.nn4.small2.v1.t7')\n",
    "\n",
    "# Load the face recognizer and label encoder\n",
    "with open('./recognizers/recognizer.pickle', 'rb') as file:\n",
    "    recognizer = pickle.load(file)\n",
    "\n",
    "with open('./recognizers/le.pickle', 'rb') as file:\n",
    "    le = pickle.load(file)\n",
    "\n",
    "# List of valid class labels\n",
    "valid_labels = ['KimWardlaw', 'JohnBOwens', 'WilliamFletcher']\n",
    "valid_indices = [i for i, label in enumerate(le.classes_) if label in valid_labels]\n",
    "\n",
    "# Open the video file\n",
    "cap = cv2.VideoCapture('11-71541 Ara Hovanesyan v. Eric Holder, Jr.-QEvMv81TIQM.mp4')\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "end_frame = min(30 * fps, int(cap.get(cv2.CAP_PROP_FRAME_COUNT)))\n",
    "\n",
    "frame_dict = {}\n",
    "frame_id = 0\n",
    "\n",
    "while(frame_id < end_frame):\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame_id += 1\n",
    "    (h, w) = frame.shape[:2]\n",
    "    top_half = frame[:h//2, :]\n",
    "\n",
    "    try:\n",
    "        gray = cv2.cvtColor(top_half, cv2.COLOR_BGR2GRAY)\n",
    "    except:\n",
    "        break\n",
    "\n",
    "    # Perform face detection\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30), flags=cv2.CASCADE_SCALE_IMAGE)\n",
    "\n",
    "    class_ids = []\n",
    "    confidences = []\n",
    "    boxes = []\n",
    "    emotions_list = []\n",
    "\n",
    "    for (startX, startY, width, height) in faces:\n",
    "        endX = startX + width\n",
    "        endY = startY + height\n",
    "\n",
    "        # Extract the face ROI\n",
    "        face = top_half[startY:endY, startX:endX]\n",
    "        (fH, fW) = face.shape[:2]\n",
    "        if fW < 20 or fH < 20:\n",
    "            continue\n",
    "\n",
    "        # Construct image blob, 128-d representation of face\n",
    "        faceBlob = cv2.dnn.blobFromImage(face, 1.0 / 255,\n",
    "                                         (96, 96), (0, 0, 0), swapRB=True, crop=False)\n",
    "\n",
    "        # Embed the 128-d representation\n",
    "        embedder.setInput(faceBlob)\n",
    "        vec = embedder.forward()\n",
    "\n",
    "        # Classify faces with most likely prediction\n",
    "        preds = recognizer.predict_proba(vec)[0]\n",
    "        filtered_preds = np.full_like(preds, fill_value=-np.inf)\n",
    "        filtered_preds[valid_indices] = preds[valid_indices]\n",
    "\n",
    "        j = np.argmax(filtered_preds)\n",
    "        proba = filtered_preds[j]\n",
    "        name = le.classes_[j]\n",
    "\n",
    "        # Placeholder for emotion detection (if available)\n",
    "        emote_text = 'none'\n",
    "\n",
    "        # Draw box, text, and emotion\n",
    "        text = \"{}: {:.2f}%, {}\".format(name, proba * 100, emote_text)\n",
    "        y = startY - 10 if startY - 10 > 10 else startY + 10\n",
    "        cv2.rectangle(top_half, (startX, startY), (endX, endY), (0, 0, 255), 2)\n",
    "        cv2.putText(top_half, text, (startX, y), cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 0, 255), 2)\n",
    "\n",
    "        class_ids.append(name)\n",
    "        confidences.append(float(proba))\n",
    "        boxes.append([startX, startY, endX, endY])\n",
    "        emotions_list.append(emote_text)\n",
    "\n",
    "        unique_ids = []\n",
    "        unique_probs = []\n",
    "        unique_boxes = []\n",
    "        unique_emotions = []\n",
    "\n",
    "        for pred, prob, box, emotion in zip(class_ids, confidences, boxes, emotions_list):\n",
    "            if pred in unique_ids:\n",
    "                idx = unique_ids.index(pred)\n",
    "                if prob > unique_probs[idx]:\n",
    "                    unique_ids[idx] = pred\n",
    "                    unique_probs[idx] = prob\n",
    "                    unique_boxes[idx] = box\n",
    "                    unique_emotions[idx] = emotion\n",
    "            else:\n",
    "                unique_ids.append(pred)\n",
    "                unique_probs.append(prob)\n",
    "                unique_boxes.append(box)\n",
    "                unique_emotions.append(emotion)\n",
    "\n",
    "        frame_info = {\n",
    "            'frame': top_half,\n",
    "            'class_ids': unique_ids,\n",
    "            'confidences': unique_probs,\n",
    "            'boxes': unique_boxes,\n",
    "            'emotions': unique_emotions\n",
    "        }\n",
    "\n",
    "        frame_dict[frame_id] = frame_info\n",
    "\n",
    "    # Show frame\n",
    "    cv2.imshow(\"frame\", top_half)\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    if key == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above approach to video reflects a primarily custom neural network based approach. But large language models have revolutionized more than just text! The latest models also are useful to analyzing images. The basic approach remains the same as before: we start by analyzing just one frame then scale up to an entire video. The main difference is that instead of standard outputs from trained neural networks and similar methods, our outputs will be generated by GPT. There are a number of advantages and disadvantages to this approach. The main advantages is that these models are <b>very</b> impressive. The disadvantage is that they generate a lot of unstructured data, and therefore require more cleanup in the backend."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With GPT, it can be helpful to break down steps so that we are not asking the model to do too many different things at once. Let's first start by seeing if we can identify the names of the judges in the first image. Let's start by getting our API key ready:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI API Key\n",
    "\n",
    "with open(\"openai_key.txt\", \"r\") as file:\n",
    "    openai_key = file.read().replace(\"\\n\", \"\")\n",
    "\n",
    "api_key = openai_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to get our image ready. The OpenAI API does not allow us to upload an image directly (unlike the user interface). So first we need to encode it using `base64`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to encode the image\n",
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "# Path to your image\n",
    "image_path = \"first_frame.jpg\"\n",
    "\n",
    "# Getting the base64 string\n",
    "base64_image = encode_image(image_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's prepare the API call. Notice the prompt here: \"What are the names of the judges in this image? Look at the text in the black square that comes after the word Before: and return the results as a list like [Judge1, Judge2, Judge3]\"\n",
    "\n",
    "Notice how we use the fact that the names of the judges are in image as text - ChatGPT vision models are quite good at optical character recognition!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "  \"Content-Type\": \"application/json\",\n",
    "  \"Authorization\": f\"Bearer {api_key}\"\n",
    "}\n",
    "\n",
    "payload = {\n",
    "  \"model\": \"gpt-4o-mini\",\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": [\n",
    "        {\n",
    "          \"type\": \"text\",\n",
    "          \"text\": \"What are the names of the judges in this image? Look at the text in the black square that comes after the word Before: and return the results as a list like [Judge1, Judge2, Judge3]\"\n",
    "        },\n",
    "        {\n",
    "          \"type\": \"image_url\",\n",
    "          \"image_url\": {\n",
    "            \"url\": f\"data:image/jpeg;base64,{base64_image}\"\n",
    "          }\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  ],\n",
    "  \"max_tokens\": 300,\n",
    "  \"temperature\": 0.0\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's make the API call and extract the content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload)\n",
    "\n",
    "response_text = response.json()['choices']\n",
    "\n",
    "response_content = response_text[0]['message']['content']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then write a regular expression to extract the names of the judges. Notice that this only works if we get the output we expect from GPT! With temperature at 0 this is a pretty safe bet, but not 100% guaranteed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all items inside square brackets\n",
    "judge_names = re.findall(r'\\[([^]]*)\\]', response_content)\n",
    "\n",
    "if judge_names:\n",
    "    # Split the first match by comma and strip spaces\n",
    "    judge_names_list = [name.strip() for name in judge_names[0].split(',')]\n",
    "else:\n",
    "    judge_names_list = []\n",
    "\n",
    "print(judge_names_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Position"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's get each judge's position. Check out the prompt this time, we tell it which 3 judges are in the picture and also note that there is an attorney presenting an argument. We then ask GPT to ask where each judge is and structure the output like [Judge1 position1, Judge2 position2, Judge3 position3]. Again, we have to hope that GPT produces output the way we want!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "  \"Content-Type\": \"application/json\",\n",
    "  \"Authorization\": f\"Bearer {api_key}\"\n",
    "}\n",
    "\n",
    "# Define the prompt\n",
    "prompt = f\"\"\"\n",
    "        The judges are identified as {judge_names_list[0]}, {judge_names_list[1]}, and {judge_names_list[2]}. An attorney is standing before them, presenting their argument. \n",
    "        \n",
    "        Where is each judge located in the image? Structure your answer like [Judge1 position1, Judge2 position2, Judge3 position3]\n",
    "        \"\"\"\n",
    "\n",
    "payload = {\n",
    "  \"model\": \"gpt-4o-mini\",\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": [\n",
    "        {\n",
    "          \"type\": \"text\",\n",
    "          \"text\": prompt\n",
    "        },\n",
    "        {\n",
    "          \"type\": \"image_url\",\n",
    "          \"image_url\": {\n",
    "            \"url\": f\"data:image/jpeg;base64,{base64_image}\"\n",
    "          }\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  ],\n",
    "  \"max_tokens\": 2000,\n",
    "  \"temperature\": 0.0\n",
    "}\n",
    "\n",
    "response = requests.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload)\n",
    "\n",
    "response_text = response.json()['choices']\n",
    "\n",
    "response_content = response_text[0]['message']['content']\n",
    "\n",
    "# Find all items inside square brackets\n",
    "judge_positions = re.findall(r'\\[([^]]*)\\]', response_content)\n",
    "\n",
    "if judge_positions:\n",
    "    # Split the first match by comma and strip spaces\n",
    "    judge_positions = [position.strip() for position in judge_positions[0].split(',')]\n",
    "else:\n",
    "    judge_positions = []\n",
    "\n",
    "print(judge_positions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's go ahead and match the judge names and positions in a dictionary that we will expand on later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "judges_dict = {'Judge1': {},\n",
    "            'Judge2': {},\n",
    "            'Judge3': {}}\n",
    "\n",
    "# Populate the dictionary\n",
    "for i, item in enumerate(judge_positions, 1):\n",
    "    name, position = item.rsplit(' ', 1)  # Split from the right to separate name and position\n",
    "    judges_dict[f'Judge{i}'] = {'name': name, 'position': position}\n",
    "\n",
    "# Print the resulting dictionary\n",
    "print(judges_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But wait! Before we go on, let's take another look at the image again. What do you notice about the order of the judges? Does it match the order of the text at the bottom, or the list we created with GPT? \n",
    "\n",
    "You might notice that the judges are sitting like [W. Fletcher, Wardlaw, Owens] but their names are ordered [Wardlaw, W. Fletcher, Owens]. So GPT cannot rely on just using the order in the text to place the judges. Despite this limitation, what do you notice about how it guessed each judge's position? Most likely, it got it right!\n",
    "\n",
    "Why does this work? GPT is good at doing zero-shot or few-shot learning. Whereas traditional machine learning relies on a lot of data to make accurate predictions, GPT benefits from the fact that it was already trained on a massive corpus of numbers, text, and images. It is therefore sometimes able to do things like infer who each judge in a frame is without explicit training examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's extract some features. Earlier we looked at things like facial emotion expression, and extracted some preliminary features that could potentially be used to estimate body posture, gaze, etc. But would it be simpler to do this with GPT? What if we prompted GPT for certain features? For example, body language, facial expression, gestures, and gaze?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "  \"Content-Type\": \"application/json\",\n",
    "  \"Authorization\": f\"Bearer {api_key}\"\n",
    "}\n",
    "\n",
    "# Define the prompt\n",
    "prompt = f\"\"\"\n",
    "    Analyze the following courtroom scene with three judges presiding over a case. \n",
    "\n",
    "    For each judge, provide the following details and structure the output as follows:\n",
    "\n",
    "    - **Judge's Position**: [left, right, center])\n",
    "    - **Body Language**:\n",
    "    - **Facial Expression**:\n",
    "    - **Gestures**: (hand movements, head nods, etc.) \n",
    "    - **Gaze**: (who or what they are looking at) \n",
    "    \"\"\"\n",
    "\n",
    "payload = {\n",
    "  \"model\": \"gpt-4o-mini\",\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": [\n",
    "        {\n",
    "          \"type\": \"text\",\n",
    "          \"text\": prompt\n",
    "        },\n",
    "        {\n",
    "          \"type\": \"image_url\",\n",
    "          \"image_url\": {\n",
    "            \"url\": f\"data:image/jpeg;base64,{base64_image}\"\n",
    "          }\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  ],\n",
    "  \"max_tokens\": 2000,\n",
    "  \"temperature\": 0.0\n",
    "}\n",
    "\n",
    "response = requests.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what the output looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.json()['choices'][0]['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like it structured the output pretty close to what we wanted, and gave us some interesting descriptions about what the judges in each position are doing! Let's go ahead and structure this output as a dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the response text\n",
    "response_text = response.json()['choices'][0]['message']['content']\n",
    "\n",
    "# Function to parse the response and structure it as a dictionary\n",
    "def extract_features(text):\n",
    "    # Regular expression for matching each judge's information in a flexible way\n",
    "    judge_pattern = re.compile(\n",
    "        r\"(?:###\\s*Judge\\s+on\\s+the\\s+|-\\s*\\*\\*Judge's\\s+Position\\*\\*:\\s*)(Left|Center|Right)\\s*[:-]?\\s*(?:\\n\\s*-\\s*\\*\\*Judge's\\s+Position\\*\\*:\\s*\\1\\s*[:-]?)?\\s*\\n\\s*-\\s*\\*\\*Body\\s+Language\\*\\*:\\s*(.*?)\\.\\s*\\n\\s*-\\s*\\*\\*Facial\\s+Expression\\*\\*:\\s*(.*?)\\.\\s*\\n\\s*-\\s*\\*\\*Gestures\\*\\*:\\s*(.*?)\\.\\s*\\n\\s*-\\s*\\*\\*Gaze\\*\\*:\\s*(.*?)\\.\",\n",
    "        re.DOTALL | re.IGNORECASE\n",
    "    )\n",
    "\n",
    "    # Extracting judges' information\n",
    "    judges = judge_pattern.findall(text)\n",
    "    \n",
    "    # Creating the dictionary to store extracted information\n",
    "    courtroom_info = {}\n",
    "    \n",
    "    # Populating the dictionary with judges' information\n",
    "    for judge in judges:\n",
    "        judge_id, body_language, facial_expression, gestures, gaze = judge\n",
    "        courtroom_info[judge_id.lower()] = {\n",
    "            'Body Language': body_language.strip(),\n",
    "            'Facial Expression': facial_expression.strip(),\n",
    "            'Gestures': gestures.strip(),\n",
    "            'Gaze': gaze.strip()\n",
    "        }\n",
    "    \n",
    "    return courtroom_info\n",
    "\n",
    "# Extracting the features\n",
    "courtroom_info = extract_features(response_text)\n",
    "\n",
    "# Printing the extracted information\n",
    "print(json.dumps(courtroom_info, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then update our original dictionary with this info:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping function to add features to the existing dictionary\n",
    "def add_features_to_existing(existing_dict, new_features):\n",
    "    for judge, info in existing_dict.items():\n",
    "        # Match position using fuzzy matching\n",
    "        position = info['position'].strip().lower()\n",
    "        matched_position, score = process.extractOne(position, new_features.keys(), scorer=fuzz.partial_ratio)\n",
    "        \n",
    "        # If the match is strong enough, update the dictionary\n",
    "        if score > 80:\n",
    "            existing_dict[judge].update(new_features[matched_position])\n",
    "    \n",
    "    return existing_dict\n",
    "\n",
    "# Updating the existing dictionary with the extracted features\n",
    "updated_dict = add_features_to_existing(judges_dict, courtroom_info)\n",
    "\n",
    "# Printing the updated dictionary\n",
    "print(json.dumps(updated_dict, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's go ahead and try this on a video. We can start by creating a dictionary that will store all of our info:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_dict = {'frame': {'id': 0, \n",
    "                       'frame_info': updated_dict}}\n",
    "\n",
    "video_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a video we need to make some additional preparations. Here we'll define functions that resize frames, extract frames from a video (skipping every nth frame so we don't waste money analyzing frames within the same second), analyze the frame based on our code for one image, and update the video dictionary with info from the new frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to resize the frame\n",
    "def resize_frame(frame, width, height):\n",
    "    return cv2.resize(frame, (width, height))\n",
    "\n",
    "# Function to extract frames from the video and resize them\n",
    "def extract_frames(video_path, frame_rate, duration, width, height):\n",
    "    video_capture = cv2.VideoCapture(video_path)\n",
    "    frame_count = 0\n",
    "    frames = []\n",
    "    total_frames = int(duration * frame_rate)\n",
    "\n",
    "    while video_capture.isOpened() and frame_count < total_frames:\n",
    "        success, frame = video_capture.read()\n",
    "        if not success:\n",
    "            break\n",
    "\n",
    "        if frame_count % frame_rate == 0:\n",
    "            frame_id = int(frame_count / frame_rate)\n",
    "            resized_frame = resize_frame(frame, width, height)\n",
    "            frame_path = f\"frame_{frame_id}.jpg\"\n",
    "            cv2.imwrite(frame_path, resized_frame)\n",
    "            frames.append((frame_id, frame_path))\n",
    "\n",
    "        frame_count += 1\n",
    "\n",
    "    video_capture.release()\n",
    "    return frames\n",
    "\n",
    "def analyze_frame(frame_path):\n",
    "    # Getting the base64 string\n",
    "    base64_image = encode_image(frame_path)\n",
    "\n",
    "    headers = {\n",
    "      \"Content-Type\": \"application/json\",\n",
    "      \"Authorization\": f\"Bearer {api_key}\"\n",
    "    }\n",
    "\n",
    "    # Define the prompt\n",
    "    prompt = f\"\"\"\n",
    "        Analyze the following courtroom scene with three judges presiding over a case. \n",
    "\n",
    "        For each judge, provide the following details and structure the output as follows:\n",
    "\n",
    "        - **Judge's Position**: [left, right, center])\n",
    "        - **Body Language**:\n",
    "        - **Facial Expression**:\n",
    "        - **Gestures**: (hand movements, head nods, etc.) \n",
    "        - **Gaze**: (who or what they are looking at) \n",
    "        \"\"\"\n",
    "\n",
    "    payload = {\n",
    "      \"model\": \"gpt-4o\",\n",
    "      \"messages\": [\n",
    "        {\n",
    "          \"role\": \"user\",\n",
    "          \"content\": [\n",
    "            {\n",
    "              \"type\": \"text\",\n",
    "              \"text\": prompt\n",
    "            },\n",
    "            {\n",
    "              \"type\": \"image_url\",\n",
    "              \"image_url\": {\n",
    "                \"url\": f\"data:image/jpeg;base64,{base64_image}\"\n",
    "              }\n",
    "            }\n",
    "          ]\n",
    "        }\n",
    "      ],\n",
    "      \"max_tokens\": 2000,\n",
    "      \"temperature\": 0.0\n",
    "    }\n",
    "\n",
    "    response = requests.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload)\n",
    "    response.raise_for_status()  # Raise an exception for HTTP errors\n",
    "    return response.json()\n",
    "\n",
    "def extract_number(filename):\n",
    "    # Regular expression to find the number\n",
    "    match = re.search(r'\\d+', filename)\n",
    "    \n",
    "    if match:\n",
    "        return match.group()\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Function to add a new frame to the video dictionary\n",
    "def add_new_frame(video_dict, file_path, new_frame_info):\n",
    "    # Extract the frame number\n",
    "    frame_number = extract_number(file_path)\n",
    "    \n",
    "    if frame_number is not None:\n",
    "        # Create the new frame entry\n",
    "        new_frame_key = 'frame' + frame_number\n",
    "        new_frame_id = int(frame_number)\n",
    "        new_frame = {\n",
    "            'id': new_frame_id,\n",
    "            'frame_info': new_frame_info\n",
    "        }\n",
    "        \n",
    "        # Update the video_dict with the new frame\n",
    "        video_dict[new_frame_key] = new_frame\n",
    "    else:\n",
    "        print(\"No frame number found in the file path.\")\n",
    "    \n",
    "    return video_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's go ahead and process the first 10 seconds of our video:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the video file\n",
    "video_path = \"11-71541 Ara Hovanesyan v. Eric Holder, Jr.-QEvMv81TIQM.mp4\"\n",
    "\n",
    "# Extract frames from the video (first 10 seconds, one frame per second, assuming 30 fps)\n",
    "frames = extract_frames(video_path, frame_rate=30, duration=10, width=640, height=480)\n",
    "\n",
    "# Dictionary to store analysis results\n",
    "#video_analysis = {}\n",
    "\n",
    "# Analyze each frame\n",
    "for frame_id, frame_path in frames:\n",
    "    try:\n",
    "        response = analyze_frame(frame_path)\n",
    "        response_text = response['choices'][0]['message']['content']\n",
    "        analysis = extract_features(response_text)\n",
    "        # Updating the existing dictionary with the extracted features\n",
    "        analysis_dict = add_features_to_existing(judges_dict, analysis)\n",
    "        add_new_frame(video_dict, frame_path, analysis_dict)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing frame {frame_id}: {e}\")\n",
    "    finally:\n",
    "        os.remove(frame_path)  # Clean up the frame file after processing\n",
    "        \n",
    "# Print the structured results\n",
    "print(\"\\nStructured Results as Python Dictionary:\")\n",
    "print(json.dumps(video_dict, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally convert the dictionary to a `pandas` dataframe!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to store rows\n",
    "rows = []\n",
    "\n",
    "# Iterate through each frame in the dictionary\n",
    "for frame_key, frame_data in video_dict.items():\n",
    "    frame_id = frame_data['id']\n",
    "    frame_info = frame_data['frame_info']\n",
    "    \n",
    "    # Extract information for each judge and flatten the dictionary\n",
    "    row = {'frame_id': frame_id}\n",
    "    for judge_key, judge_data in frame_info.items():\n",
    "        for info_key, info_value in judge_data.items():\n",
    "            row[f\"{judge_key}_{info_key}\"] = info_value\n",
    "    \n",
    "    # Append the row to the list\n",
    "    rows.append(row)\n",
    "\n",
    "# Create a DataFrame from the list of rows\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "# Display the DataFrame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow! We now have data about what the judges are doing in the first 10 seconds of the video. Now that the data is structured by judge, and the values are text, we can use some of our standard computer science and statistics toolkit to analyze the data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Material"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Color "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complexity"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
